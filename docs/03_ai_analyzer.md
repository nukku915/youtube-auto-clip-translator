# AIåˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è©³ç´°è¨ˆç”»æ›¸

## 1. æ¦‚è¦

### ç›®çš„
ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMæ§‹æˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ« + ã‚¯ãƒ©ã‚¦ãƒ‰ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æãƒ»ç¿»è¨³ã‚’è¡Œã†

### LLMãƒ—ãƒ­ãƒã‚¤ãƒ€
| ãƒ—ãƒ­ãƒã‚¤ãƒ€ | ç”¨é€” | ãƒ¢ãƒ‡ãƒ« |
|-----------|------|--------|
| Ollamaï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ï¼‰ | è¦‹ã©ã“ã‚æ¤œå‡ºã€ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡º | gemma-2-jpn:2b |
| Geminiï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰ï¼‰ | ç¿»è¨³ã€ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ | gemini-3-flash |

### è²¬å‹™
- ãƒ†ã‚­ã‚¹ãƒˆã®ç¿»è¨³ï¼ˆæ—¥æœ¬èªâ‡”è‹±èªã€è‡ªå‹•æ¤œå‡ºï¼‰
- è¦‹ã©ã“ã‚æ¤œå‡ºãƒ»ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
- ãƒãƒ£ãƒ—ã‚¿ãƒ¼åŒºåˆ‡ã‚Šæ¤œå‡º
- ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
- LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ã®è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
- APIã‚³ã‚¹ãƒˆæœ€é©åŒ–

---

## 2. ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ

```
AI Analyzer
â”œâ”€â”€ Translator          # ç¿»è¨³
â”œâ”€â”€ HighlightDetector   # è¦‹ã©ã“ã‚æ¤œå‡º
â”œâ”€â”€ ChapterDetector     # ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡º
â””â”€â”€ TitleGenerator      # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ
```

---

## 3. å…±é€šä»•æ§˜

### ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMè¨­å®š

```python
@dataclass
class LLMConfig:
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMè¨­å®š"""
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€è¨­å®š
    provider: str = "hybrid"  # "local", "gemini", "hybrid"

    # ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaï¼‰è¨­å®š
    local_enabled: bool = True
    local_model: str = "gemma-2-jpn:2b"
    ollama_host: str = "http://localhost:11434"
    ollama_timeout: int = 120  # ãƒ­ãƒ¼ã‚«ãƒ«ã¯æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§

    # ã‚¯ãƒ©ã‚¦ãƒ‰LLMï¼ˆGeminiï¼‰è¨­å®š
    gemini_enabled: bool = True
    gemini_api_key: str = ""
    gemini_model: str = "gemini-3-flash"
    gemini_timeout: int = 60

    # å…±é€šè¨­å®š
    temperature: float = 0.3
    max_output_tokens: int = 8192

    # ã‚¿ã‚¹ã‚¯æŒ¯ã‚Šåˆ†ã‘
    use_local_for: List[str] = field(default_factory=lambda: [
        "highlight_detection",
        "chapter_detection",
    ])
    use_gemini_for: List[str] = field(default_factory=lambda: [
        "translation",
        "title_generation",
    ])

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    fallback_to_gemini: bool = True

    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼ˆGeminiç”¨ï¼‰
    requests_per_minute: int = 60
    retry_count: int = 3
    retry_delay: float = 1.0
```

### Ollamaè¨­å®š

```python
@dataclass
class OllamaConfig:
    host: str = "http://localhost:11434"
    model: str = "gemma-2-jpn:2b"
    timeout: int = 120
    temperature: float = 0.3
    num_ctx: int = 8192  # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚º
```

### Geminiè¨­å®š

```python
@dataclass
class GeminiConfig:
    api_key: str                          # APIã‚­ãƒ¼
    model: str = "gemini-3-flash"         # ãƒ¢ãƒ‡ãƒ«å
    temperature: float = 0.3              # ç”Ÿæˆæ¸©åº¦
    max_output_tokens: int = 8192         # æœ€å¤§å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³
    timeout: int = 60                     # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰

    # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    requests_per_minute: int = 60
    retry_count: int = 3
    retry_delay: float = 1.0
```

### è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«

```yaml
# ~/.youtube-auto-clip-translator/config.yaml
llm:
  provider: "hybrid"  # hybrid, local, gemini
  fallback_to_gemini: true

  ollama:
    host: "http://localhost:11434"
    model: "gemma-2-jpn:2b"
    timeout: 120

  gemini:
    api_key: "YOUR_API_KEY_HERE"
    model: "gemini-3-flash"
    timeout: 60

  task_routing:
    highlight_detection: "local"
    chapter_detection: "local"
    translation: "gemini"
    title_generation: "gemini"
```

**èª­ã¿è¾¼ã¿å„ªå…ˆé †ä½ï¼ˆGemini API Keyï¼‰**:
1. ç’°å¢ƒå¤‰æ•° `GEMINI_API_KEY`ï¼ˆè¨­å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
2. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ« `~/.youtube-auto-clip-translator/config.yaml`

### LLMãƒ«ãƒ¼ã‚¿ãƒ¼

```python
class LLMRouter:
    """ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ã‚’é¸æŠ"""

    def __init__(self, config: LLMConfig):
        self.config = config
        self.ollama_client = OllamaClient(config) if config.local_enabled else None
        self.gemini_client = GeminiClient(config) if config.gemini_enabled else None

    async def execute(
        self,
        task: str,
        prompt: str,
        **kwargs
    ) -> LLMResponse:
        """
        ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦é©åˆ‡ãªLLMã§å®Ÿè¡Œ

        Args:
            task: ã‚¿ã‚¹ã‚¯ç¨®åˆ¥ (highlight_detection, translation, etc.)
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        Returns:
            LLMResponse: å¿œç­”
        """
        provider = self._get_provider_for_task(task)

        try:
            if provider == "local":
                return await self.ollama_client.generate(prompt, **kwargs)
            else:
                return await self.gemini_client.generate(prompt, **kwargs)
        except Exception as e:
            if self.config.fallback_to_gemini and provider == "local":
                # ãƒ­ãƒ¼ã‚«ãƒ«å¤±æ•—æ™‚ã¯Geminiã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return await self.gemini_client.generate(prompt, **kwargs)
            raise

    def _get_provider_for_task(self, task: str) -> str:
        if task in self.config.use_local_for and self.config.local_enabled:
            return "local"
        return "gemini"
```

### æ¥ç¶šãƒ†ã‚¹ãƒˆ

ä¿å­˜æ™‚ã«APIã‚­ãƒ¼ã®æœ‰åŠ¹æ€§ã¨Ollamaã®æ¥ç¶šã‚’ç¢ºèª

### å…±é€šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿
```python
@dataclass
class AnalysisInput:
    segments: List[TranscriptionSegment]  # æ–‡å­—èµ·ã“ã—çµæœ
    source_language: str                   # å…ƒè¨€èª
    video_metadata: VideoMetadata          # å‹•ç”»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
```

---

## 4. ç¿»è¨³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (Translator)

### 4.1 å…¥å‡ºåŠ›ä»•æ§˜

#### å…¥åŠ›
| é …ç›® | å‹ | å¿…é ˆ | èª¬æ˜ |
|------|-----|------|------|
| segments | List[TranscriptionSegment] | Yes | ç¿»è¨³å¯¾è±¡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ |
| source_lang | str | No | å…ƒè¨€èªï¼ˆè‡ªå‹•æ¤œå‡ºå¯ï¼‰ |
| target_lang | str | Yes | ç¿»è¨³å…ˆè¨€èª |

#### å‡ºåŠ›
```python
@dataclass
class TranslationResult:
    translated_segments: List[TranslatedSegment]
    source_language: str
    target_language: str
    total_tokens_used: int

@dataclass
class TranslatedSegment:
    id: int                    # å…ƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆID
    original_text: str         # åŸæ–‡
    translated_text: str       # è¨³æ–‡
    start: float               # é–‹å§‹æ™‚é–“
    end: float                 # çµ‚äº†æ™‚é–“
    words: List[TranslatedWord]  # å˜èªãƒ¬ãƒ™ãƒ«ç¿»è¨³ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
```

### 4.2 ç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ

```python
TRANSLATION_SYSTEM_PROMPT = """
ã‚ãªãŸã¯å‹•ç”»å­—å¹•ã®ç¿»è¨³å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ç¿»è¨³ã—ã¦ãã ã•ã„ï¼š

## ãƒ«ãƒ¼ãƒ«
1. è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„è¨³æ–‡ã«ã™ã‚‹
2. å­—å¹•ã¨ã—ã¦è¡¨ç¤ºã•ã‚Œã‚‹ãŸã‚ã€1ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ40æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã™ã‚‹
3. è©±ã—è¨€è‘‰ã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’ç¶­æŒã™ã‚‹
4. å°‚é–€ç”¨èªã¯æ–‡è„ˆã«å¿œã˜ã¦é©åˆ‡ã«è¨³ã™
5. å›ºæœ‰åè©ï¼ˆäººåã€åœ°åã€ãƒ–ãƒ©ãƒ³ãƒ‰åï¼‰ã¯åŸå‰‡ãã®ã¾ã¾
6. ç›¸æ§Œã‚„æ„Ÿå˜†è©ã‚‚è‡ªç„¶ã«è¨³ã™

## å‡ºåŠ›å½¢å¼
JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{
  "translations": [
    {"id": 1, "text": "è¨³æ–‡"},
    {"id": 2, "text": "è¨³æ–‡"}
  ]
}
"""

TRANSLATION_USER_PROMPT = """
ä»¥ä¸‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’{source_lang}ã‹ã‚‰{target_lang}ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä¸€è¦§ï¼š
{segments_json}
"""
```

### 4.3 ãƒãƒƒãƒå‡¦ç†

ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ãŸã‚ã€è¤‡æ•°ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã¾ã¨ã‚ã¦ç¿»è¨³ï¼š

```python
BATCH_CONFIG = {
    "max_segments_per_request": 50,   # 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®æœ€å¤§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°
    "max_tokens_per_request": 4000,   # 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ãŸã‚Šã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
}
```

### 4.4 å‡¦ç†ãƒ•ãƒ­ãƒ¼

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒåˆ†å‰²                                â”‚
â”‚    â””â”€ ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è€ƒæ…®ã—ã¦é©åˆ‡ãªã‚µã‚¤ã‚ºã«åˆ†å‰²             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å„ãƒãƒƒãƒã‚’ä¸¦åˆ—ã§APIå‘¼ã³å‡ºã—                           â”‚
â”‚    â””â”€ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ãŸä¸¦åˆ—åº¦åˆ¶å¾¡                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒ‘ãƒ¼ã‚¹ãƒ»æ¤œè¨¼                              â”‚
â”‚    â””â”€ JSONå½¢å¼ã®æ¤œè¨¼ã€æ¬ æãƒã‚§ãƒƒã‚¯                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. å…ƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¨ã®ãƒãƒƒãƒ”ãƒ³ã‚°                            â”‚
â”‚    â””â”€ ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®å¼•ãç¶™ã                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. è¦‹ã©ã“ã‚æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (HighlightDetector)

### 5.1 å…¥å‡ºåŠ›ä»•æ§˜

#### å…¥åŠ›
| é …ç›® | å‹ | å¿…é ˆ | èª¬æ˜ |
|------|-----|------|------|
| segments | List[TranscriptionSegment] | Yes | åˆ†æå¯¾è±¡ |
| video_type | str | No | å‹•ç”»ã‚¿ã‚¤ãƒ—ï¼ˆgaming, talk, tutorialç­‰ï¼‰ |

#### å‡ºåŠ›
```python
@dataclass
class HighlightResult:
    highlights: List[Highlight]
    summary: str  # å‹•ç”»å…¨ä½“ã®è¦ç´„

@dataclass
class Highlight:
    start: float           # é–‹å§‹æ™‚é–“
    end: float             # çµ‚äº†æ™‚é–“
    score: float           # è¦‹ã©ã“ã‚ã‚¹ã‚³ã‚¢ï¼ˆ0-100ï¼‰
    reason: str            # é¸å‡ºç†ç”±
    category: str          # ã‚«ãƒ†ã‚´ãƒªï¼ˆfunny, informative, dramaticç­‰ï¼‰
    suggested_title: str   # åˆ‡ã‚ŠæŠœãã‚¿ã‚¤ãƒˆãƒ«å€™è£œ
```

### 5.2 è¦‹ã©ã“ã‚æ¤œå‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
HIGHLIGHT_SYSTEM_PROMPT = """
ã‚ãªãŸã¯å‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚
æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€Œè¦‹ã©ã“ã‚ã€ã¨ãªã‚‹éƒ¨åˆ†ã‚’æ¤œå‡ºã—ã¦ãã ã•ã„ã€‚

## è¦‹ã©ã“ã‚ã®åˆ¤æ–­åŸºæº–
1. **é¢ç™½ã„å ´é¢**: ç¬‘ã„ã‚’èª˜ã†ç™ºè¨€ã€ãƒ¦ãƒ¼ãƒ¢ã‚¢
2. **é‡è¦ãªæƒ…å ±**: æ ¸å¿ƒçš„ãªè§£èª¬ã€çµè«–
3. **æ„Ÿæƒ…çš„ãªãƒ”ãƒ¼ã‚¯**: é©šãã€æ„Ÿå‹•ã€èˆˆå¥®
4. **å°è±¡çš„ãªãƒ•ãƒ¬ãƒ¼ã‚º**: åè¨€ã€ã‚­ãƒ£ãƒƒãƒãƒ¼ãªç™ºè¨€
5. **ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹**: ç››ã‚Šä¸ŠãŒã‚Šã€è»¢æ›ç‚¹

## å‡ºåŠ›å½¢å¼
JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{
  "highlights": [
    {
      "start_segment_id": 10,
      "end_segment_id": 15,
      "score": 85,
      "reason": "é¸å‡ºç†ç”±",
      "category": "funny",
      "suggested_title": "ã€çˆ†ç¬‘ã€‘..."
    }
  ],
  "summary": "å‹•ç”»å…¨ä½“ã®è¦ç´„ï¼ˆ100å­—ç¨‹åº¦ï¼‰"
}
"""
```

### 5.3 ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–

| ã‚¹ã‚³ã‚¢ | åˆ¤å®š | èª¬æ˜ |
|--------|------|------|
| 90-100 | å¿…è¦‹ | å‹•ç”»ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã€å¿…ãšå«ã‚ã‚‹ã¹ã |
| 70-89 | é‡è¦ | é«˜ã„ä¾¡å€¤ãŒã‚ã‚Šã€å«ã‚ã‚‹ã“ã¨ã‚’æ¨å¥¨ |
| 50-69 | æ™®é€š | å«ã‚ã¦ã‚‚è‰¯ã„ãŒå¿…é ˆã§ã¯ãªã„ |
| 0-49 | ä½ | åˆ‡ã‚ŠæŠœãã«ã¯ä¸å‘ã |

---

## 6. ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡ºãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (ChapterDetector)

### 6.1 å…¥å‡ºåŠ›ä»•æ§˜

#### å‡ºåŠ›
```python
@dataclass
class ChapterResult:
    chapters: List[Chapter]

@dataclass
class Chapter:
    id: int                # ãƒãƒ£ãƒ—ã‚¿ãƒ¼ID
    start: float           # é–‹å§‹æ™‚é–“
    end: float             # çµ‚äº†æ™‚é–“
    title: str             # ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã‚¿ã‚¤ãƒˆãƒ«
    summary: str           # ãƒãƒ£ãƒ—ã‚¿ãƒ¼è¦ç´„
    segment_ids: List[int] # å«ã¾ã‚Œã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆID
```

### 6.2 ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
CHAPTER_SYSTEM_PROMPT = """
ã‚ãªãŸã¯å‹•ç”»æ§‹æˆã®åˆ†æå°‚é–€å®¶ã§ã™ã€‚
æ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒˆãƒ”ãƒƒã‚¯ã®å¤‰åŒ–ã‚’æ¤œå‡ºã—ã€ãƒãƒ£ãƒ—ã‚¿ãƒ¼åˆ†ã‘ã—ã¦ãã ã•ã„ã€‚

## ãƒãƒ£ãƒ—ã‚¿ãƒ¼åˆ†ã‘ã®åŸºæº–
1. **è©±é¡Œã®è»¢æ›**: æ–°ã—ã„ãƒˆãƒ”ãƒƒã‚¯ã¸ã®ç§»è¡Œ
2. **å ´é¢ã®å¤‰åŒ–**: çŠ¶æ³èª¬æ˜ã®å¤‰åŒ–
3. **æ§‹æˆã®åŒºåˆ‡ã‚Š**: å°å…¥â†’æœ¬é¡Œâ†’ã¾ã¨ã‚ ãªã©
4. **æ™‚é–“ã®çµŒé**: ã€Œæ¬¡ã¯ã€ã€Œç¶šã„ã¦ã€ãªã©ã®æ¥ç¶š

## å‡ºåŠ›å½¢å¼
JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
{
  "chapters": [
    {
      "start_segment_id": 1,
      "end_segment_id": 20,
      "title": "ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°",
      "summary": "ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã®æ¦‚è¦ï¼ˆ50å­—ç¨‹åº¦ï¼‰"
    }
  ]
}
"""
```

---

## 7. ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (TitleGenerator)

### 7.1 å…¥å‡ºåŠ›ä»•æ§˜

#### å‡ºåŠ›
```python
@dataclass
class TitleResult:
    main_title: str                  # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«
    alternative_titles: List[str]    # ä»£æ›¿ã‚¿ã‚¤ãƒˆãƒ«å€™è£œ
    tags: List[str]                  # æ¨å¥¨ã‚¿ã‚°
    description: str                 # èª¬æ˜æ–‡

@dataclass
class SectionTitle:
    chapter_id: int
    title: str                       # è¡¨ç¤ºç”¨ã‚¿ã‚¤ãƒˆãƒ«
    style: str                       # ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆbold, highlightç­‰ï¼‰
```

### 7.2 ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```python
TITLE_SYSTEM_PROMPT = """
ã‚ãªãŸã¯YouTubeå‹•ç”»ã®ã‚¿ã‚¤ãƒˆãƒ«ä½œæˆå°‚é–€å®¶ã§ã™ã€‚
è¦–è´è€…ã®èˆˆå‘³ã‚’å¼•ãã€ã‚¯ãƒªãƒƒã‚¯ç‡ã®é«˜ã„ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

## ã‚¿ã‚¤ãƒˆãƒ«ã®è¦ä»¶
1. 30æ–‡å­—ä»¥å†…
2. å†…å®¹ã‚’çš„ç¢ºã«è¡¨ç¾
3. èˆˆå‘³ã‚’å¼•ããƒ•ãƒƒã‚¯
4. ä¸è¦ãªè£…é£¾ã¯é¿ã‘ã‚‹

## Shortsç”¨ã‚¿ã‚¤ãƒˆãƒ«
- 15æ–‡å­—ä»¥å†…
- ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆé‡è¦–

## å‡ºåŠ›å½¢å¼
{
  "main_title": "ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«",
  "alternative_titles": ["å€™è£œ1", "å€™è£œ2", "å€™è£œ3"],
  "shorts_title": "Shortsç”¨ã‚¿ã‚¤ãƒˆãƒ«",
  "tags": ["ã‚¿ã‚°1", "ã‚¿ã‚°2"],
  "description": "å‹•ç”»ã®èª¬æ˜æ–‡ï¼ˆ200å­—ç¨‹åº¦ï¼‰"
}
"""
```

---

## 8. ã‚³ã‚¹ãƒˆæœ€é©åŒ–æˆ¦ç•¥

### 8.1 å‡¦ç†é †åºã®æœ€é©åŒ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: åˆ†æï¼ˆä½ã‚³ã‚¹ãƒˆï¼‰                                â”‚
â”‚ â”œâ”€ è¦‹ã©ã“ã‚æ¤œå‡º                                         â”‚
â”‚ â”œâ”€ ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡º                                       â”‚
â”‚ â””â”€ ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ                                         â”‚
â”‚                                                         â”‚
â”‚ â€» ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®å‡¦ç†ãªã®ã§ãƒˆãƒ¼ã‚¯ãƒ³æ¶ˆè²»ãŒå°‘ãªã„           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: äººé–“ã«ã‚ˆã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼                              â”‚
â”‚ â””â”€ åˆ‡ã‚ŠæŠœãç¯„å›²ã®ç¢ºå®š                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: ç¿»è¨³ï¼ˆç¢ºå®šéƒ¨åˆ†ã®ã¿ï¼‰                            â”‚
â”‚ â””â”€ ã‚³ã‚¹ãƒˆæœ€å°åŒ–                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è¦‹ç©ã‚‚ã‚Š

| å‡¦ç† | å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ | å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ | å‚™è€ƒ |
|------|-------------|-------------|------|
| è¦‹ã©ã“ã‚æ¤œå‡º | ~2000 | ~500 | å‹•ç”»å…¨ä½“ã‚’1å›åˆ†æ |
| ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡º | ~2000 | ~300 | å‹•ç”»å…¨ä½“ã‚’1å›åˆ†æ |
| ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ | ~500 | ~200 | ãƒãƒ£ãƒ—ã‚¿ãƒ¼ã”ã¨ |
| ç¿»è¨³ | ~100/ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ | ~100/ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ | ç¢ºå®šéƒ¨åˆ†ã®ã¿ |

### 8.3 ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥

```python
@dataclass
class AnalysisCache:
    video_id: str
    transcript_hash: str     # æ–‡å­—èµ·ã“ã—çµæœã®ãƒãƒƒã‚·ãƒ¥
    highlights: Optional[HighlightResult]
    chapters: Optional[ChapterResult]
    translations: Dict[str, TranslationResult]  # lang -> result
    created_at: datetime
    expires_at: datetime
```

---

## 9. Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ

### OllamaClient ã‚¯ãƒ©ã‚¹

```python
class OllamaClient:
    """Ollamaãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ç”¨ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, config: OllamaConfig):
        self.config = config
        self.base_url = config.host

    async def generate(
        self,
        prompt: str,
        system_prompt: str = None,
        **kwargs
    ) -> LLMResponse:
        """
        ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

        Args:
            prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            system_prompt: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

        Returns:
            LLMResponse: å¿œç­”
        """
        payload = {
            "model": self.config.model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": self.config.temperature,
                "num_ctx": self.config.num_ctx,
            }
        }

        if system_prompt:
            payload["system"] = system_prompt

        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            ) as response:
                result = await response.json()
                return LLMResponse(
                    text=result["response"],
                    provider="ollama",
                    model=self.config.model,
                )

    async def is_available(self) -> bool:
        """OllamaãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{self.base_url}/api/tags",
                    timeout=aiohttp.ClientTimeout(total=5)
                ) as response:
                    return response.status == 200
        except Exception:
            return False

    async def list_models(self) -> List[str]:
        """ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{self.base_url}/api/tags") as response:
                data = await response.json()
                return [m["name"] for m in data.get("models", [])]

    async def pull_model(
        self,
        model: str,
        progress_callback: Callable[[float, str], None] = None
    ) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/api/pull",
                json={"name": model, "stream": True}
            ) as response:
                async for line in response.content:
                    data = json.loads(line)
                    if "completed" in data and "total" in data:
                        progress = data["completed"] / data["total"] * 100
                        if progress_callback:
                            progress_callback(progress, f"Downloading {model}...")
                    if data.get("status") == "success":
                        return True
        return False
```

### LLMResponse ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹

```python
@dataclass
class LLMResponse:
    text: str                    # å¿œç­”ãƒ†ã‚­ã‚¹ãƒˆ
    provider: str                # ãƒ—ãƒ­ãƒã‚¤ãƒ€å (ollama/gemini)
    model: str                   # ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«å
    tokens_used: int = 0         # ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆGeminiã®ã¿ï¼‰
    latency_ms: float = 0.0      # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
```

---

## 10. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### ã‚¨ãƒ©ãƒ¼ç¨®åˆ¥
| ã‚¨ãƒ©ãƒ¼ | åŸå›  | å¯¾å‡¦ |
|--------|------|------|
| `APIKeyError` | ç„¡åŠ¹ãªAPIã‚­ãƒ¼ï¼ˆGeminiï¼‰ | ã‚­ãƒ¼ã®å†è¨­å®šã‚’ä¿ƒã™ |
| `RateLimitError` | ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆGeminiï¼‰ | æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤ |
| `QuotaExceededError` | ä½¿ç”¨é‡ä¸Šé™ï¼ˆGeminiï¼‰ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é€šçŸ¥ |
| `InvalidResponseError` | ä¸æ­£ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ | ãƒªãƒˆãƒ©ã‚¤ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ |
| `TimeoutError` | ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ | ãƒªãƒˆãƒ©ã‚¤ |
| `OllamaNotRunningError` | Ollamaã‚µãƒ¼ãƒ“ã‚¹æœªèµ·å‹• | è‡ªå‹•èµ·å‹•è©¦è¡Œ or ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ |
| `ModelNotFoundError` | ãƒ¢ãƒ‡ãƒ«æœªãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ | ãƒ¢ãƒ‡ãƒ«DLä¿ƒã™ or ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ |
| `OllamaConnectionError` | Ollamaæ¥ç¶šã‚¨ãƒ©ãƒ¼ | Geminiã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ |

### ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥
```python
RETRY_CONFIG = {
    "max_retries": 3,
    "base_delay": 1.0,
    "max_delay": 60.0,
    "exponential_base": 2,
    "jitter": True,
}
```

---

## 10. ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
src/core/ai_analyzer/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ analyzer.py           # ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹: AIAnalyzer
â”œâ”€â”€ translator.py         # ç¿»è¨³: Translator
â”œâ”€â”€ highlight_detector.py # è¦‹ã©ã“ã‚æ¤œå‡º: HighlightDetector
â”œâ”€â”€ chapter_detector.py   # ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡º: ChapterDetector
â”œâ”€â”€ title_generator.py    # ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ: TitleGenerator
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router.py         # LLMãƒ«ãƒ¼ã‚¿ãƒ¼ï¼ˆã‚¿ã‚¹ã‚¯æŒ¯ã‚Šåˆ†ã‘ï¼‰
â”‚   â”œâ”€â”€ base.py           # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”œâ”€â”€ ollama_client.py  # Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”‚   â””â”€â”€ gemini_client.py  # Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ translation.py
â”‚   â”œâ”€â”€ highlight.py
â”‚   â”œâ”€â”€ chapter.py
â”‚   â””â”€â”€ title.py
â”œâ”€â”€ models.py             # ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ config.py             # è¨­å®šï¼ˆLLMConfigå«ã‚€ï¼‰
â”œâ”€â”€ cache.py              # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
â””â”€â”€ exceptions.py         # ã‚«ã‚¹ã‚¿ãƒ ä¾‹å¤–
```

---

## 11. ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©

### AIAnalyzer ã‚¯ãƒ©ã‚¹
```python
class AIAnalyzer:
    def __init__(self, config: LLMConfig):
        """
        åˆæœŸåŒ–

        Args:
            config: ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMè¨­å®š
        """
        self.config = config
        self.router = LLMRouter(config)

    async def analyze_full(
        self,
        input: AnalysisInput,
        progress_callback: Callable[[float, str], None] = None
    ) -> FullAnalysisResult:
        """
        å…¨åˆ†æã‚’å®Ÿè¡Œï¼ˆè¦‹ã©ã“ã‚ + ãƒãƒ£ãƒ—ã‚¿ãƒ¼ + ã‚¿ã‚¤ãƒˆãƒ«ï¼‰

        Returns:
            FullAnalysisResult: å…¨åˆ†æçµæœ
        """

    async def detect_highlights(
        self,
        segments: List[TranscriptionSegment]
    ) -> HighlightResult:
        """è¦‹ã©ã“ã‚æ¤œå‡ºã®ã¿"""

    async def detect_chapters(
        self,
        segments: List[TranscriptionSegment]
    ) -> ChapterResult:
        """ãƒãƒ£ãƒ—ã‚¿ãƒ¼æ¤œå‡ºã®ã¿"""

    async def translate(
        self,
        segments: List[TranscriptionSegment],
        target_lang: str
    ) -> TranslationResult:
        """ç¿»è¨³ã®ã¿"""

    async def generate_titles(
        self,
        chapters: List[Chapter],
        context: str
    ) -> TitleResult:
        """ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã®ã¿"""

    def estimate_cost(
        self,
        segments: List[TranscriptionSegment]
    ) -> CostEstimate:
        """å‡¦ç†ã‚³ã‚¹ãƒˆã®è¦‹ç©ã‚‚ã‚Š"""
```

---

## 12. ä½¿ç”¨ä¾‹

```python
from core.ai_analyzer import AIAnalyzer, LLMConfig

# ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰LLMè¨­å®š
config = LLMConfig(
    provider="hybrid",

    # ãƒ­ãƒ¼ã‚«ãƒ«LLMè¨­å®š
    local_enabled=True,
    local_model="gemma-2-jpn:2b",
    ollama_host="http://localhost:11434",

    # ã‚¯ãƒ©ã‚¦ãƒ‰LLMè¨­å®š
    gemini_enabled=True,
    gemini_api_key="YOUR_API_KEY",
    gemini_model="gemini-3-flash",

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ‰åŠ¹
    fallback_to_gemini=True,
)

analyzer = AIAnalyzer(config)

# å…¨åˆ†æå®Ÿè¡Œ
result = await analyzer.analyze_full(
    input=AnalysisInput(
        segments=transcription_result.segments,
        source_language="en",
        video_metadata=video_metadata
    ),
    progress_callback=lambda p, s: print(f"[{p:.1f}%] {s}")
)

# è¦‹ã©ã“ã‚ç¢ºèª
for highlight in result.highlights.highlights:
    print(f"[{highlight.start:.1f}s - {highlight.end:.1f}s]")
    print(f"  Score: {highlight.score}")
    print(f"  Reason: {highlight.reason}")

# ç¢ºå®šã—ãŸéƒ¨åˆ†ã®ã¿ç¿»è¨³ï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼‰
selected_segment_ids = [1, 2, 3, 10, 11, 12]  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠ
selected_segments = [s for s in segments if s.id in selected_segment_ids]

translation = await analyzer.translate(
    segments=selected_segments,
    target_lang="ja"
)
```

---

## 13. ãƒ†ã‚¹ãƒˆé …ç›®

### ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
- [ ] ç¿»è¨³ç²¾åº¦ï¼ˆæ—¥â†’è‹±ã€è‹±â†’æ—¥ï¼‰
- [ ] è¦‹ã©ã“ã‚æ¤œå‡ºã®å¦¥å½“æ€§
- [ ] ãƒãƒ£ãƒ—ã‚¿ãƒ¼åŒºåˆ‡ã‚Šã®é©åˆ‡ã•
- [ ] ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã®å“è³ª
- [ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‘ãƒ¼ã‚¹

### çµ±åˆãƒ†ã‚¹ãƒˆ
- [ ] é•·æ™‚é–“å‹•ç”»ï¼ˆ1æ™‚é–“ä»¥ä¸Šï¼‰ã®å‡¦ç†
- [ ] å¤šè¨€èªæ··åœ¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
- [ ] ã‚¨ãƒ©ãƒ¼ãƒªã‚«ãƒãƒª
- [ ] ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥å‹•ä½œ

---

## 14. è¿½åŠ ä»•æ§˜

### 14.1 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤‰æ›´æ™‚ã®äº’æ›æ€§ã‚’ç¢ºä¿ï¼š

```python
@dataclass
class PromptVersion:
    version: str           # "1.0.0"
    created_at: datetime
    description: str
    template: str
    expected_output_schema: dict

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¬ã‚¸ã‚¹ãƒˆãƒª
PROMPT_REGISTRY = {
    "translation": {
        "1.0.0": PromptVersion(
            version="1.0.0",
            created_at=datetime(2026, 1, 19),
            description="åˆç‰ˆç¿»è¨³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            template=TRANSLATION_SYSTEM_PROMPT_V1,
            expected_output_schema={
                "type": "object",
                "properties": {
                    "translations": {"type": "array"}
                }
            }
        ),
        "1.1.0": PromptVersion(
            version="1.1.0",
            created_at=datetime(2026, 1, 20),
            description="æ–‡è„ˆè€ƒæ…®ã‚’è¿½åŠ ",
            template=TRANSLATION_SYSTEM_PROMPT_V1_1,
            expected_output_schema={...}
        ),
    },
    "highlight_detection": {...},
    "chapter_detection": {...},
    "title_generation": {...},
}

class PromptManager:
    def get_prompt(self, task: str, version: str = "latest") -> PromptVersion:
        """æŒ‡å®šãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        versions = PROMPT_REGISTRY.get(task, {})
        if version == "latest":
            return versions[max(versions.keys())]
        return versions[version]

    def validate_output(self, output: dict, prompt: PromptVersion) -> bool:
        """å‡ºåŠ›ãŒã‚¹ã‚­ãƒ¼ãƒã«æº–æ‹ ã—ã¦ã„ã‚‹ã‹æ¤œè¨¼"""
        import jsonschema
        try:
            jsonschema.validate(output, prompt.expected_output_schema)
            return True
        except jsonschema.ValidationError:
            return False
```

### 14.2 ç¿»è¨³å“è³ªæ¤œè¨¼

```python
@dataclass
class TranslationQualityCheck:
    segment_id: int
    original_length: int
    translated_length: int
    length_ratio: float
    has_untranslated: bool      # åŸæ–‡ãŒæ®‹ã£ã¦ã„ã‚‹
    has_placeholder: bool       # [ç¿»è¨³ä¸å¯] ãªã©ãŒå«ã¾ã‚Œã‚‹
    confidence_score: float     # 0.0-1.0

def validate_translation(
    original: str,
    translated: str,
    source_lang: str,
    target_lang: str
) -> TranslationQualityCheck:
    """ç¿»è¨³å“è³ªã‚’æ¤œè¨¼"""
    checks = TranslationQualityCheck(...)

    # é•·ã•æ¯”ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µç«¯ãªå·®ç•°ã¯å•é¡Œã®å¯èƒ½æ€§ï¼‰
    ratio = len(translated) / len(original) if original else 0
    if source_lang == "en" and target_lang == "ja":
        # è‹±â†’æ—¥ã¯é€šå¸¸0.5ã€œ1.5å€
        if ratio < 0.3 or ratio > 2.0:
            checks.confidence_score *= 0.5

    # åŸæ–‡æ®‹ç•™ãƒã‚§ãƒƒã‚¯
    if source_lang == "en" and re.search(r'[a-zA-Z]{5,}', translated):
        checks.has_untranslated = True
        checks.confidence_score *= 0.7

    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ãƒã‚§ãƒƒã‚¯
    if re.search(r'\[.*ä¸å¯.*\]|\[.*error.*\]', translated, re.I):
        checks.has_placeholder = True
        checks.confidence_score = 0.0

    return checks

async def translate_with_quality_check(
    segments: List[TranscriptionSegment],
    target_lang: str,
    quality_threshold: float = 0.7
) -> TranslationResult:
    """å“è³ªãƒã‚§ãƒƒã‚¯ä»˜ãç¿»è¨³"""
    result = await translate(segments, target_lang)

    low_quality_segments = []
    for translated in result.translated_segments:
        check = validate_translation(
            translated.original_text,
            translated.translated_text,
            result.source_language,
            target_lang
        )
        if check.confidence_score < quality_threshold:
            low_quality_segments.append(translated.id)
            translated.flags.append("low_quality_translation")

    if low_quality_segments:
        result.warnings.append(
            f"{len(low_quality_segments)}ä»¶ã®ç¿»è¨³å“è³ªãŒä½ã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™"
        )

    return result
```

### 14.3 ã‚³ã‚¹ãƒˆç›£è¦–æ©Ÿèƒ½

```python
@dataclass
class UsageRecord:
    timestamp: datetime
    task: str                # translation, highlight, etc.
    provider: str            # gemini, ollama
    model: str
    input_tokens: int
    output_tokens: int
    estimated_cost_usd: float
    video_id: str

class CostMonitor:
    """APIä½¿ç”¨é‡ã¨ã‚³ã‚¹ãƒˆã®ç›£è¦–"""

    def __init__(self, db_path: Path):
        self.db_path = db_path

    def record_usage(self, record: UsageRecord) -> None:
        """ä½¿ç”¨é‡ã‚’è¨˜éŒ²"""
        ...

    def get_daily_usage(self, date: datetime = None) -> dict:
        """æ—¥æ¬¡ä½¿ç”¨é‡ã‚’å–å¾—"""
        return {
            "total_requests": 150,
            "total_input_tokens": 50000,
            "total_output_tokens": 15000,
            "estimated_cost_usd": 0.15,
            "breakdown_by_task": {...}
        }

    def get_monthly_usage(self, month: int, year: int) -> dict:
        """æœˆæ¬¡ä½¿ç”¨é‡ã‚’å–å¾—"""
        ...

    def check_quota(self, estimated_tokens: int) -> QuotaStatus:
        """ã‚¯ã‚©ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯"""
        daily_limit = 1_000_000  # ãƒˆãƒ¼ã‚¯ãƒ³
        current_usage = self.get_daily_usage()["total_input_tokens"]

        return QuotaStatus(
            within_limit=current_usage + estimated_tokens < daily_limit,
            current_usage=current_usage,
            limit=daily_limit,
            estimated_cost=self._estimate_cost(estimated_tokens)
        )

    def _estimate_cost(self, tokens: int) -> float:
        """ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šï¼ˆGemini Flashæ–™é‡‘ï¼‰"""
        # $0.075 / 1M input tokens (2026å¹´1æœˆæ™‚ç‚¹)
        return tokens * 0.075 / 1_000_000

# è¨­å®šç”»é¢ã§ã®è¡¨ç¤º
"""
ğŸ“Š APIä½¿ç”¨çŠ¶æ³
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä»Šæœˆã®ä½¿ç”¨é‡:
  ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: 1,234 å›
  ãƒˆãƒ¼ã‚¯ãƒ³: 2.5M
  æ¨å®šã‚³ã‚¹ãƒˆ: $0.19

æ—¥æ¬¡åˆ¶é™: 1M ãƒˆãƒ¼ã‚¯ãƒ³
ä»Šæ—¥ã®ä½¿ç”¨: 150K (15%)
"""
```

### 14.4 Ollamaèµ·å‹•å¤±æ•—æ™‚ã®å¯¾å‡¦

```python
class OllamaManager:
    """Ollamaã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†"""

    async def ensure_running(self) -> bool:
        """OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã€å¿…è¦ãªã‚‰èµ·å‹•"""
        if await self.is_running():
            return True

        return await self.start()

    async def is_running(self) -> bool:
        """OllamaãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèª"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    "http://localhost:11434/api/tags",
                    timeout=aiohttp.ClientTimeout(total=2)
                ) as response:
                    return response.status == 200
        except:
            return False

    async def start(self) -> bool:
        """Ollamaã‚’èµ·å‹•"""
        import subprocess
        import sys

        try:
            if sys.platform == "darwin":
                # macOS: ã‚¢ãƒ—ãƒªã‚’èµ·å‹•
                subprocess.Popen(["open", "-a", "Ollama"])
            elif sys.platform == "win32":
                # Windows: ã‚µãƒ¼ãƒ“ã‚¹ã‚’èµ·å‹•
                subprocess.Popen(["ollama", "serve"])
            else:
                # Linux: systemctl or ç›´æ¥èµ·å‹•
                subprocess.Popen(["ollama", "serve"])

            # èµ·å‹•å¾…æ©Ÿï¼ˆæœ€å¤§30ç§’ï¼‰
            for _ in range(30):
                await asyncio.sleep(1)
                if await self.is_running():
                    return True

            return False
        except Exception as e:
            logger.error(f"Ollama startup failed: {e}")
            return False

    async def ensure_model(self, model: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€ãªã‘ã‚Œã°ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        models = await self.list_models()
        if model in models:
            return True

        return await self.pull_model(model)

# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ•ãƒ­ãƒ¼
async def execute_with_ollama_fallback(task: str, prompt: str):
    manager = OllamaManager()

    # 1. Ollamaèµ·å‹•ç¢ºèª
    if not await manager.ensure_running():
        if config.fallback_to_gemini:
            logger.warning("Ollama unavailable, falling back to Gemini")
            return await gemini_client.generate(prompt)
        raise OllamaNotRunningError("Ollamaã‚’èµ·å‹•ã§ãã¾ã›ã‚“ã§ã—ãŸ")

    # 2. ãƒ¢ãƒ‡ãƒ«ç¢ºèª
    if not await manager.ensure_model(config.local_model):
        raise OllamaModelNotFoundError(f"ãƒ¢ãƒ‡ãƒ« {config.local_model} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # 3. å®Ÿè¡Œ
    return await ollama_client.generate(prompt)
```

### 14.5 ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã®å¯¾å‡¦

```python
@dataclass
class ParseResult:
    success: bool
    data: Optional[dict]
    raw_text: str
    error: Optional[str]
    retry_recommended: bool

def parse_llm_response(response: str, expected_schema: dict) -> ParseResult:
    """LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆå …ç‰¢æ€§å‘ä¸Šç‰ˆï¼‰"""

    # 1. æ¨™æº–JSONãƒ‘ãƒ¼ã‚¹
    try:
        data = json.loads(response)
        return ParseResult(success=True, data=data, raw_text=response, error=None, retry_recommended=False)
    except json.JSONDecodeError:
        pass

    # 2. ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONæŠ½å‡º
    json_match = re.search(r'```(?:json)?\s*([\s\S]*?)```', response)
    if json_match:
        try:
            data = json.loads(json_match.group(1))
            return ParseResult(success=True, data=data, raw_text=response, error=None, retry_recommended=False)
        except:
            pass

    # 3. éƒ¨åˆ†çš„ãªJSONæŠ½å‡ºï¼ˆ{...}ã‚’æ¢ã™ï¼‰
    brace_match = re.search(r'\{[\s\S]*\}', response)
    if brace_match:
        try:
            data = json.loads(brace_match.group(0))
            return ParseResult(success=True, data=data, raw_text=response, error=None, retry_recommended=False)
        except:
            pass

    # 4. é…åˆ—ã®æŠ½å‡º
    bracket_match = re.search(r'\[[\s\S]*\]', response)
    if bracket_match:
        try:
            data = json.loads(bracket_match.group(0))
            return ParseResult(success=True, data={"items": data}, raw_text=response, error=None, retry_recommended=False)
        except:
            pass

    # 5. ãƒ‘ãƒ¼ã‚¹å¤±æ•—
    return ParseResult(
        success=False,
        data=None,
        raw_text=response,
        error="JSONã‚’ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸ",
        retry_recommended=True
    )

async def execute_with_parse_retry(
    prompt: str,
    expected_schema: dict,
    max_retries: int = 2
) -> dict:
    """ãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã«ãƒªãƒˆãƒ©ã‚¤"""
    for attempt in range(max_retries + 1):
        response = await llm_client.generate(prompt)
        result = parse_llm_response(response.text, expected_schema)

        if result.success:
            return result.data

        if attempt < max_retries and result.retry_recommended:
            # ãƒªãƒˆãƒ©ã‚¤æ™‚ã¯æ˜ç¤ºçš„ãªæŒ‡ç¤ºã‚’è¿½åŠ 
            prompt = f"{prompt}\n\né‡è¦: å¿…ãšæœ‰åŠ¹ãªJSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚èª¬æ˜æ–‡ã¯ä¸è¦ã§ã™ã€‚"
            continue

        # æœ€çµ‚è©¦è¡Œã§ã‚‚å¤±æ•—
        raise InvalidResponseError(
            f"LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸ: {result.error}",
            raw_response=result.raw_text
        )
```

### 14.6 éƒ¨åˆ†çš„æˆåŠŸã®æ‰±ã„

100ã‚»ã‚°ãƒ¡ãƒ³ãƒˆä¸­10å€‹ã ã‘å¤±æ•—ã—ãŸå ´åˆãªã©ã®å¯¾å‡¦ï¼š

```python
@dataclass
class PartialResult:
    successful: List[TranslatedSegment]
    failed: List[FailedSegment]
    success_rate: float
    can_proceed: bool  # ç¶šè¡Œå¯èƒ½ã‹

@dataclass
class FailedSegment:
    segment_id: int
    original_text: str
    error: str
    retryable: bool

async def translate_with_partial_success(
    segments: List[TranscriptionSegment],
    target_lang: str,
    min_success_rate: float = 0.9
) -> PartialResult:
    """éƒ¨åˆ†çš„æˆåŠŸã‚’è¨±å®¹ã™ã‚‹ç¿»è¨³"""
    successful = []
    failed = []

    # ãƒãƒƒãƒå‡¦ç†
    for batch in chunk(segments, 50):
        try:
            result = await translate_batch(batch, target_lang)
            successful.extend(result.translated_segments)
        except Exception as e:
            # ãƒãƒƒãƒå…¨ä½“ãŒå¤±æ•—ã—ãŸå ´åˆã€å€‹åˆ¥ã«ãƒªãƒˆãƒ©ã‚¤
            for segment in batch:
                try:
                    result = await translate_single(segment, target_lang)
                    successful.append(result)
                except Exception as e2:
                    failed.append(FailedSegment(
                        segment_id=segment.id,
                        original_text=segment.text,
                        error=str(e2),
                        retryable=is_retryable_error(e2)
                    ))

    success_rate = len(successful) / len(segments)

    return PartialResult(
        successful=successful,
        failed=failed,
        success_rate=success_rate,
        can_proceed=success_rate >= min_success_rate
    )

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®é€šçŸ¥
"""
âš ï¸ ä¸€éƒ¨ã®ç¿»è¨³ã«å¤±æ•—ã—ã¾ã—ãŸ

æˆåŠŸ: 95/100 ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ (95%)
å¤±æ•—: 5 ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ

å¤±æ•—ã—ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯åŸæ–‡ã®ã¾ã¾è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
å­—å¹•ç·¨é›†ç”»é¢ã§æ‰‹å‹•ä¿®æ­£ã§ãã¾ã™ã€‚

[ç¶šè¡Œ] [å†è©¦è¡Œ] [ã‚­ãƒ£ãƒ³ã‚»ãƒ«]
"""
```

### 14.7 ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·è¶…éã®å¯¾å‡¦

```python
@dataclass
class ChunkingConfig:
    max_tokens_per_request: int = 4000
    overlap_segments: int = 2  # æ–‡è„ˆç¶­æŒã®ãŸã‚ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

def estimate_tokens(text: str) -> int:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’æ¦‚ç®—ï¼ˆæ—¥æœ¬èªã¯æ–‡å­—æ•°Ã—1.5ã€è‹±èªã¯å˜èªæ•°Ã—1.3ï¼‰"""
    # ç°¡æ˜“æ¨å®š
    if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', text):
        return int(len(text) * 1.5)
    else:
        return int(len(text.split()) * 1.3)

def chunk_segments_by_tokens(
    segments: List[TranscriptionSegment],
    config: ChunkingConfig
) -> List[List[TranscriptionSegment]]:
    """ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åŸºã¥ã„ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
    chunks = []
    current_chunk = []
    current_tokens = 0

    for segment in segments:
        segment_tokens = estimate_tokens(segment.text)

        if current_tokens + segment_tokens > config.max_tokens_per_request:
            # æ–°ã—ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é–‹å§‹
            if current_chunk:
                chunks.append(current_chunk)
            # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
            overlap = current_chunk[-config.overlap_segments:] if current_chunk else []
            current_chunk = overlap + [segment]
            current_tokens = sum(estimate_tokens(s.text) for s in current_chunk)
        else:
            current_chunk.append(segment)
            current_tokens += segment_tokens

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

async def translate_long_content(
    segments: List[TranscriptionSegment],
    target_lang: str
) -> TranslationResult:
    """é•·ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†å‰²ã—ã¦ç¿»è¨³"""
    config = ChunkingConfig()
    chunks = chunk_segments_by_tokens(segments, config)

    all_translated = []
    for i, chunk in enumerate(chunks):
        # æ–‡è„ˆæƒ…å ±ã‚’è¿½åŠ 
        context = ""
        if i > 0:
            context = f"å‰ã®æ–‡è„ˆ: ã€Œ{all_translated[-1].translated_text}ã€\n"

        result = await translate_batch(chunk, target_lang, context=context)
        all_translated.extend(result.translated_segments)

    # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—éƒ¨åˆ†ã®é‡è¤‡ã‚’è§£æ¶ˆ
    seen_ids = set()
    deduplicated = []
    for segment in all_translated:
        if segment.id not in seen_ids:
            seen_ids.add(segment.id)
            deduplicated.append(segment)

    return TranslationResult(translated_segments=deduplicated, ...)
```

---

## æ›´æ–°å±¥æ­´

| æ—¥ä»˜ | å†…å®¹ |
|------|------|
| 2026-01-19 | åˆç‰ˆä½œæˆ |
| 2026-01-19 | è¿½åŠ ä»•æ§˜ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ã€å“è³ªæ¤œè¨¼ã€ã‚³ã‚¹ãƒˆç›£è¦–ç­‰ï¼‰ã‚’è¿½è¨˜ |
