#!/usr/bin/env python3
"""
YouTubeåˆ‡ã‚ŠæŠœã + è©±è€…è­˜åˆ¥ + éŸ³å£°èªè­˜ + è‡ªå‹•å­¦ç¿’ + Geminiç¿»è¨³ ã‚·ã‚¹ãƒ†ãƒ 

ä½¿ç”¨æ–¹æ³•:
    # é€šå¸¸å‡¦ç†ï¼ˆæŒ‡å®šåŒºé–“ã‚’åˆ‡ã‚ŠæŠœãï¼‰
    python clip_with_speaker.py <YouTube URL> <é–‹å§‹æ™‚é–“> <çµ‚äº†æ™‚é–“>

    # Geminiç¿»è¨³ã‚’ä½¿ç”¨ï¼ˆé«˜ç²¾åº¦ï¼‰
    python clip_with_speaker.py <YouTube URL> <é–‹å§‹æ™‚é–“> <çµ‚äº†æ™‚é–“> --gemini

    # ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºï¼ˆé•·ã„å‹•ç”»ã‹ã‚‰ç››ã‚Šä¸ŠãŒã‚Šéƒ¨åˆ†ã‚’æ¤œå‡ºï¼‰
    python clip_with_speaker.py <YouTube URL> --detect

    # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆå‡¦ç†å¾Œã«ç¢ºèªãƒ»ä¿®æ­£ï¼‰
    python clip_with_speaker.py <YouTube URL> <é–‹å§‹æ™‚é–“> <çµ‚äº†æ™‚é–“> --learn

    # ç›´è¿‘ã®çµæœã‚’ä¿®æ­£ã—ã¦å­¦ç¿’
    python clip_with_speaker.py --correct

    # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å“è³ªãƒã‚§ãƒƒã‚¯ï¼†è‡ªå‹•æ”¹å–„
    python clip_with_speaker.py --improve T1

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
    --team T1       ãƒãƒ¼ãƒ ã‚’æŒ‡å®šï¼ˆT1, GenG, HLE, DK, KTï¼‰
    --output xxx    å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    --gemini        Gemini APIã§é«˜ç²¾åº¦ç¿»è¨³
    --detect        ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰
    --learn         å‡¦ç†å¾Œã«å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã‚’èµ·å‹•
    --auto-learn    è‡ªå‹•å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆé«˜ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰
    --correct       ç›´è¿‘ã®çµæœã‚’ä¿®æ­£ã—ã¦å­¦ç¿’
    --improve TEAM  æŒ‡å®šãƒãƒ¼ãƒ ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ”¹å–„
"""

import sys
import os
import re
import subprocess
import argparse
import numpy as np
import torchaudio
import torch
import json
import whisper
import threading
from datetime import datetime
from pathlib import Path
import yt_dlp
from googletrans import Translator
from moviepy import VideoFileClip, TextClip, CompositeVideoClip
from speechbrain.inference.speaker import EncoderClassifier
from lol_dictionary import correct_text, create_correction_dict, correct_korean_asr, correct_japanese_post

# è‡ªå‹•åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from auto_collect_voice import (
        collect_with_diarization,
        load_existing_embeddings,
        create_backup,
        list_backups,
        restore_backup,
        restore_player_embedding
    )
    AUTO_COLLECT_AVAILABLE = True
except ImportError:
    AUTO_COLLECT_AVAILABLE = False

# DemucséŸ³å£°åˆ†é›¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    from demucs.pretrained import get_model
    from demucs.apply import apply_model
    DEMUCS_AVAILABLE = True
except ImportError:
    DEMUCS_AVAILABLE = False

# è¨­å®š
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
EMBEDDINGS_DIR = os.path.join(BASE_DIR, 'data/speaker_embeddings_v2')
CACHE_DIR = os.path.join(BASE_DIR, 'data/speechbrain_cache')
TEMP_DIR = os.path.join(BASE_DIR, 'downloads/temp')
OUTPUT_DIR = os.path.expanduser('~/Downloads')
DB_PATH = os.path.join(BASE_DIR, 'data/speaker_database.json')
HISTORY_PATH = os.path.join(BASE_DIR, 'data/learning_history.json')
CUSTOM_DICT_PATH = os.path.join(BASE_DIR, 'data/custom_lol_terms.json')

# è‡ªå‹•æ”¹å–„è¨­å®š
AUTO_IMPROVE_CONFIG = {
    'enabled': True,  # å“è³ªãŒä½ã„å ´åˆã«è‡ªå‹•ã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ”¹å–„
    'threshold': 0.5,  # ä½å“è³ªã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒã“ã®å‰²åˆã‚’è¶…ãˆãŸã‚‰æ”¹å–„
    'cooldown': 3600,  # åŒã˜ãƒãƒ¼ãƒ ã®æ”¹å–„é–“éš”ï¼ˆç§’ï¼‰
}
AUTO_IMPROVE_LOG_PATH = os.path.join(BASE_DIR, 'data/auto_improve_log.json')

# Geminiç¿»è¨³ç”¨ã®LoLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
LOL_TRANSLATION_CONTEXT = """ã‚ãªãŸã¯League of Legends (LoL)ã®ãƒ—ãƒ­ãƒãƒ¼ãƒ ã®ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ï¼ˆè©¦åˆä¸­ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ã‚’ç¿»è¨³ã™ã‚‹ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã™ã€‚

## é‡è¦ãªLoLç”¨èª
### ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³
- ì˜¤ë¦¬/ì˜¤ë¦¬ì•„ë‚˜ = ã‚ªãƒªã‚¢ãƒŠ
- ê·¸ì›¬/ê°€ìœ„ = ã‚°ã‚¦ã‚§ãƒ³ï¼ˆê°€ìœ„ã¯ã‚°ã‚¦ã‚§ãƒ³ã®Qã‚¹ã‚­ãƒ«ã€Œãƒã‚µãƒŸã€ï¼‰
- ê·¸ë¼ê°€ìŠ¤/ê·¸ë¼ê²Ÿ = ã‚°ãƒ©ã‚¬ã‚¹
- ì• ë‹ˆ/ì–¸ë‹ˆ = ã‚¢ãƒ‹ãƒ¼ï¼ˆì–¸ë‹ˆã€Œå§‰ã•ã‚“ã€ã¯ã‚¢ãƒ‹ãƒ¼ã®æ„›ç§°ï¼‰
- ë¦¬ì‹ /ë¦¬ ì‹  = ãƒªãƒ¼ãƒ»ã‚·ãƒ³
- ì•¼ìŠ¤ì˜¤/ì•¼ìŠ¤ = ãƒ¤ã‚¹ã‚ª
- ìš”ë„¤ = ãƒ¨ãƒ

### ã‚¢ã‚¤ãƒ†ãƒ 
- ì¡´ì•¼ = ã‚¾ãƒ¼ãƒ‹ãƒ£ã®ç ‚æ™‚è¨ˆ
- ê°€ì—”/ìˆ˜í˜¸ì²œì‚¬ = ã‚¬ãƒ¼ãƒ‡ã‚£ã‚¢ãƒ³ã‚¨ãƒ³ã‚¸ã‚§ãƒ« (GA)
- ì–¼ì–´ë¶™ì€ ì‹¬ì¥ = ãƒ•ãƒ­ãƒ¼ã‚ºãƒ³ãƒãƒ¼ãƒˆ

### ã‚²ãƒ¼ãƒ ç”¨èª
- ë°”ë¡  = ãƒãƒ­ãƒ³
- ë“œë˜ê³¤/ìš© = ãƒ‰ãƒ©ã‚´ãƒ³
- í•œíƒ€ = é›†å›£æˆ¦
- ê°± = ã‚¬ãƒ³ã‚¯
- í•©ë¥˜ = åˆæµ
- êµ¬ë„ = æ§‹å›³/ãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
- ì½œ = ã‚³ãƒ¼ãƒ«ï¼ˆå ±å‘Šï¼‰
- ê¶ = ã‚¦ãƒ«ãƒˆ
- ë³¼ = ãƒœãƒ¼ãƒ«ï¼ˆã‚ªãƒªã‚¢ãƒŠã®ã‚¹ã‚­ãƒ«ï¼‰
- ë”œ = ãƒ€ãƒ¡ãƒ¼ã‚¸
- íƒ±í‚¹ = ã‚¿ãƒ³ã‚­ãƒ³ã‚°

### ã‚µãƒ¢ãƒŠãƒ¼ã‚¹ãƒšãƒ«
- ì ë©¸/í”Œë˜ì‹œ = ãƒ•ãƒ©ãƒƒã‚·ãƒ¥
- ì í™” = ã‚¤ã‚°ãƒŠã‚¤ãƒˆ
- ê°•íƒ€ = ã‚¹ãƒã‚¤ãƒˆ
- í…”í¬ = ãƒ†ãƒ¬ãƒãƒ¼ãƒˆ

## ç¿»è¨³ãƒ«ãƒ¼ãƒ«
1. ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ã¯çŸ­ãç°¡æ½”ã«ï¼ˆè©±ã—è¨€è‘‰ã§ï¼‰
2. ã‚²ãƒ¼ãƒ ç”¨èªã¯ã‚«ã‚¿ã‚«ãƒŠã§ãã®ã¾ã¾ä½¿ç”¨
3. æ„Ÿæƒ…ã‚„ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç¶­æŒ
4. ã€Œã§ã™/ã¾ã™ã€ã¯ä½¿ã‚ãšã€ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªå£èª¿ã§
5. æ—¥æœ¬èªç¿»è¨³ã®ã¿ã‚’å‡ºåŠ›ï¼ˆèª¬æ˜ä¸è¦ï¼‰
6. **ä¼šè©±ã®å—ã‘ç­”ãˆã‚’æ„è­˜**ï¼šè³ªå•â†’å›ç­”ã€ææ¡ˆâ†’è³›å¦ã€å ±å‘Šâ†’ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æµã‚Œã‚’è‡ªç„¶ã«
7. è¿”äº‹ãƒ»ç›¸æ§Œã¯è‡ªç„¶ãªæ—¥æœ¬èªã§ï¼ˆì•Œì•˜ì–´â†’ãŠã£ã‘/äº†è§£ã€ì‘â†’ã†ã‚“ã€ê·¸ë˜â†’ãã†ã ã­ï¼‰"""

# ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹æ¤œå‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
TEAMVOICE_DETECT_PROMPT = """Analyze the following transcript from a League of Legends (LOL) game video.

Your task is to identify moments where TEAM MEMBERS are communicating with each other during gameplay.

Look for:
- Player callouts and communication (e.g., "è¡Œãã", "ä¸‹ãŒã£ã¦", "ãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãªã„", "ultè¡Œã")
- Reactions during fights (e.g., "ãƒŠã‚¤ã‚¹!", "ã‚„ã°ã„", "ã†ã‚ãƒ¼", laughing, excited shouts)
- Team coordination moments (e.g., "ãƒ‰ãƒ©ã‚´ãƒ³å–ã‚ã†", "ãƒãƒ­ãƒ³", "ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦")
- Emotional reactions (hype moments, frustration, celebration after kills)

DO NOT include:
- Calm analytical commentary or explanations of the game
- Tutorial-style explanations
- Post-game analysis discussions

Transcript:
{transcript}

Return a JSON array of team voice moments with this exact format:
[
  {{
    "start": <start_time_in_seconds>,
    "end": <end_time_in_seconds>,
    "title": "<short description of the moment>",
    "description": "<what's happening - callout, reaction, coordination, etc.>",
    "type": "<type: callout|reaction|coordination|celebration|other>",
    "score": <importance/intensity score 0.0-1.0, higher = more energetic/important>
  }}
]

Requirements:
- Find all moments with team communication or reactions
- Each clip should be 5-30 seconds long
- Prioritize energetic, exciting, or funny moments
- Return ONLY valid JSON, no other text"""


def time_to_sec(t: str) -> int:
    """æ™‚é–“æ–‡å­—åˆ—ã‚’ç§’ã«å¤‰æ›"""
    parts = t.split(":")
    if len(parts) == 2:
        return int(parts[0]) * 60 + int(parts[1])
    return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])


def sec_to_time(sec: float) -> str:
    """ç§’ã‚’æ™‚é–“æ–‡å­—åˆ—ã«å¤‰æ›"""
    m, s = divmod(int(sec), 60)
    return f"{m}:{s:02d}"


def load_database():
    """é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿"""
    with open(DB_PATH, 'r') as f:
        return json.load(f)


def save_database(db):
    """é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
    with open(DB_PATH, 'w') as f:
        json.dump(db, f, indent=2, ensure_ascii=False)


def load_history():
    """å­¦ç¿’å±¥æ­´ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(HISTORY_PATH):
        with open(HISTORY_PATH, 'r') as f:
            return json.load(f)
    return {'sessions': []}


def save_history(history):
    """å­¦ç¿’å±¥æ­´ã‚’ä¿å­˜"""
    with open(HISTORY_PATH, 'w') as f:
        json.dump(history, f, indent=2, ensure_ascii=False)


def load_custom_dict():
    """ã‚«ã‚¹ã‚¿ãƒ LoLç”¨èªè¾æ›¸ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(CUSTOM_DICT_PATH):
        with open(CUSTOM_DICT_PATH, 'r') as f:
            return json.load(f)
    return {'terms': {}, 'learned_count': 0}


def save_custom_dict(custom_dict):
    """ã‚«ã‚¹ã‚¿ãƒ LoLç”¨èªè¾æ›¸ã‚’ä¿å­˜"""
    with open(CUSTOM_DICT_PATH, 'w') as f:
        json.dump(custom_dict, f, indent=2, ensure_ascii=False)


def get_merged_corrections():
    """åŸºæœ¬è¾æ›¸ã¨ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã‚’çµ±åˆ"""
    corrections = create_correction_dict()
    custom = load_custom_dict()
    corrections.update(custom.get('terms', {}))
    return corrections


def apply_lol_corrections(text: str) -> str:
    """LoLç”¨èªã‚’ä¿®æ­£"""
    corrections = get_merged_corrections()
    return correct_text(text, corrections)


def get_gemini_api_key():
    """Gemini APIã‚­ãƒ¼ã‚’å–å¾—"""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿è©¦è¡Œ
        env_path = os.path.join(BASE_DIR, '.env')
        if os.path.exists(env_path):
            with open(env_path, 'r') as f:
                for line in f:
                    if line.startswith('GEMINI_API_KEY='):
                        api_key = line.strip().split('=', 1)[1].strip('"\'')
                        break
    return api_key


def translate_with_gemini(text: str, speaker: str = "") -> str:
    """Gemini APIã§ç¿»è¨³ï¼ˆLoLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãï¼‰"""
    api_key = get_gemini_api_key()
    if not api_key:
        print("   âš ï¸ GEMINI_API_KEYæœªè¨­å®šã€googletransã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
        return None

    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')

        speaker_context = f"ï¼ˆè©±è€…: {speaker}ï¼‰" if speaker else ""

        prompt = f"""{LOL_TRANSLATION_CONTEXT}

## ç¿»è¨³å¯¾è±¡
éŸ“å›½èª: {text}
{speaker_context}

## å‡ºåŠ›
æ—¥æœ¬èªç¿»è¨³ã®ã¿:"""

        response = model.generate_content(prompt)
        result = response.text.strip()

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è£…é£¾ã‚’é™¤å»
        result = re.sub(r'\*\*(.+?)\*\*', r'\1', result)
        result = re.sub(r'ã€Œ(.+?)ã€', r'\1', result)

        # è¤‡æ•°è¡Œã®å ´åˆã¯æœ€åˆã®è¡Œã®ã¿
        lines = result.split('\n')
        return lines[0].strip() if lines else result

    except Exception as e:
        print(f"   âš ï¸ Geminiç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def detect_team_from_title(title: str) -> str:
    """å‹•ç”»ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ãƒãƒ¼ãƒ ã‚’æ¤œå‡º"""
    title_upper = title.upper()
    teams = ['T1', 'GENG', 'GEN.G', 'HLE', 'DK', 'KT', 'DRX', 'NS', 'BRO', 'LSB']
    team_map = {'GENG': 'GenG', 'GEN.G': 'GenG', 'T1': 'T1', 'HLE': 'HLE', 'DK': 'DK', 'KT': 'KT'}

    if ' VS ' in title_upper or ' VS.' in title_upper:
        for team in teams:
            idx = title_upper.find(team)
            vs_idx = title_upper.find(' VS')
            if idx != -1 and idx < vs_idx:
                return team_map.get(team, team)

    for team in teams:
        if team in title_upper:
            return team_map.get(team, team)

    return None


def download_clip(url: str, start_sec: int = None, end_sec: int = None) -> tuple:
    """YouTubeå‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåŒºé–“æŒ‡å®šå¯ï¼‰"""
    os.makedirs(TEMP_DIR, exist_ok=True)
    clip_path = os.path.join(TEMP_DIR, 'clip.mp4')

    title = None
    ydl_opts = {
        'format': 'best[ext=mp4]/best',
        'outtmpl': clip_path,
        'quiet': True,
        'no_warnings': True,
    }

    # åŒºé–“æŒ‡å®šãŒã‚ã‚‹å ´åˆ
    if start_sec is not None and end_sec is not None:
        ydl_opts['download_ranges'] = lambda info, ydl: [{'start_time': start_sec, 'end_time': end_sec}]
        ydl_opts['force_keyframes_at_cuts'] = True

    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=False)
        title = info.get('title', 'Unknown')
        ydl.download([url])

    return clip_path, title


def extract_audio(video_path: str) -> str:
    """å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡º"""
    audio_path = os.path.join(TEMP_DIR, 'audio.wav')
    subprocess.run([
        'ffmpeg', '-y', '-i', video_path,
        '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', audio_path
    ], capture_output=True)
    return audio_path


def separate_vocals(audio_path: str) -> str:
    """Demucsã§éŸ³å£°ã‹ã‚‰ãƒœãƒ¼ã‚«ãƒ«ã‚’åˆ†é›¢ï¼ˆã‚²ãƒ¼ãƒ éŸ³ã‚’é™¤å»ï¼‰"""
    if not DEMUCS_AVAILABLE:
        print("   âš ï¸ Demucsæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚éŸ³å£°åˆ†é›¢ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        print("   ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install demucs")
        return audio_path

    try:
        print("   ğŸµ éŸ³å£°åˆ†é›¢ä¸­ï¼ˆDemucsï¼‰...")

        # éŸ³å£°èª­ã¿è¾¼ã¿ï¼ˆ44.1kHzå¿…è¦ï¼‰
        waveform, sr = torchaudio.load(audio_path)

        # 44.1kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆDemucsã®è¦ä»¶ï¼‰
        if sr != 44100:
            resampler = torchaudio.transforms.Resample(sr, 44100)
            waveform = resampler(waveform)

        # ã‚¹ãƒ†ãƒ¬ã‚ªã«å¤‰æ›
        if waveform.shape[0] == 1:
            waveform = waveform.repeat(2, 1)

        # Demucsãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = get_model('htdemucs')
        model.eval()

        # GPUä½¿ç”¨å¯èƒ½ãªã‚‰GPU
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model.to(device)
        waveform = waveform.to(device)

        # ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ 
        waveform = waveform.unsqueeze(0)

        # éŸ³å£°åˆ†é›¢å®Ÿè¡Œ
        with torch.no_grad():
            sources = apply_model(model, waveform, device=device)

        # ãƒœãƒ¼ã‚«ãƒ«ãƒˆãƒ©ãƒƒã‚¯æŠ½å‡º (index 3 = vocals)
        vocals = sources[0, 3]  # [2, samples]

        # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
        vocals_mono = vocals.mean(dim=0, keepdim=True)

        # 16kHzã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        resampler_down = torchaudio.transforms.Resample(44100, 16000)
        vocals_16k = resampler_down(vocals_mono.cpu())

        # ä¿å­˜
        vocals_path = os.path.join(TEMP_DIR, 'vocals.wav')
        torchaudio.save(vocals_path, vocals_16k, 16000)

        print("   âœ… éŸ³å£°åˆ†é›¢å®Œäº†")
        return vocals_path

    except Exception as e:
        print(f"   âš ï¸ éŸ³å£°åˆ†é›¢ã‚¨ãƒ©ãƒ¼: {e}")
        return audio_path


def load_speaker_embeddings(team: str = None, db: dict = None) -> dict:
    """è©±è€…ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿"""
    embeddings = {}

    if team and db and team in db['teams']:
        team_players = [p.lower() for p in db['teams'][team]['players']]
        for f in os.listdir(EMBEDDINGS_DIR):
            if f.endswith('.npy'):
                name = f.replace('.npy', '')
                if name in team_players:
                    embeddings[name] = np.load(os.path.join(EMBEDDINGS_DIR, f))
    else:
        for f in os.listdir(EMBEDDINGS_DIR):
            if f.endswith('.npy'):
                name = f.replace('.npy', '')
                embeddings[name] = np.load(os.path.join(EMBEDDINGS_DIR, f))

    return embeddings


def transcribe_audio(audio_path: str, language: str = "ko", model_size: str = "base") -> list:
    """Whisperã§éŸ³å£°èªè­˜

    Args:
        audio_path: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        language: è¨€èªã‚³ãƒ¼ãƒ‰
        model_size: ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º (tiny, base, small, medium, large)
    """
    print(f"   Whisper {model_size}ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨")
    model = whisper.load_model(model_size)
    result = model.transcribe(audio_path, language=language)
    return result['segments']


def translate_to_japanese(text: str, speaker: str = "", use_gemini: bool = False,
                         use_papago: bool = False,
                         context_before: str = "", context_after: str = "") -> str:
    """éŸ“å›½èªã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã€LoLç”¨èªã‚’ä¿®æ­£

    Args:
        text: ç¿»è¨³å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
        speaker: è©±è€…å
        use_gemini: Gemini APIã‚’ä½¿ç”¨ã™ã‚‹ã‹
        use_papago: Papago APIã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆæ¨å¥¨ï¼‰
        context_before: å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡è„ˆç”¨ï¼‰
        context_after: å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ–‡è„ˆç”¨ï¼‰
    """
    if not text or len(text.strip()) == 0:
        return text

    # Papagoç¿»è¨³ã‚’è©¦è¡Œï¼ˆéŸ“æ—¥ç‰¹åŒ–ã€æœ€é«˜ç²¾åº¦ï¼‰
    if use_papago:
        try:
            from papago_translator import translate_korean_to_japanese
            result = translate_korean_to_japanese(text)
            if result and result != text:
                return apply_lol_corrections(result)
        except Exception as e:
            print(f"   âš ï¸ Papagoç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")

    # Geminiç¿»è¨³ã‚’è©¦è¡Œï¼ˆæ–‡è„ˆä»˜ãï¼‰
    if use_gemini:
        result = translate_with_gemini_context(text, speaker, context_before, context_after)
        if result:
            return apply_lol_corrections(result)

    # googletransã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    try:
        translator = Translator()
        result = translator.translate(text, src='ko', dest='ja')
        translated = result.text
        return apply_lol_corrections(translated)
    except Exception as e:
        return text


def translate_with_gemini_context(text: str, speaker: str = "",
                                   context_before: str = "", context_after: str = "") -> str:
    """Gemini APIã§æ–‡è„ˆä»˜ãç¿»è¨³"""
    api_key = get_gemini_api_key()
    if not api_key:
        return None

    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')

        speaker_context = f"è©±è€…: {speaker}" if speaker else ""

        # æ–‡è„ˆã‚’æ§‹ç¯‰
        context_parts = []
        if context_before:
            context_parts.append(f"å‰ã®ç™ºè¨€: {context_before}")
        if context_after:
            context_parts.append(f"å¾Œã®ç™ºè¨€: {context_after}")
        context_str = "\n".join(context_parts) if context_parts else ""

        prompt = f"""{LOL_TRANSLATION_CONTEXT}

## ç¿»è¨³å¯¾è±¡
{speaker_context}
éŸ“å›½èª: {text}

{f"## æ–‡è„ˆï¼ˆå‚è€ƒï¼‰" if context_str else ""}
{context_str}

## å‡ºåŠ›
æ—¥æœ¬èªç¿»è¨³ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰:"""

        response = model.generate_content(prompt)
        result = response.text.strip()

        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³è£…é£¾ã‚’é™¤å»
        result = re.sub(r'\*\*(.+?)\*\*', r'\1', result)
        result = re.sub(r'ã€Œ(.+?)ã€', r'\1', result)

        # è¤‡æ•°è¡Œã®å ´åˆã¯æœ€åˆã®è¡Œã®ã¿
        lines = result.split('\n')
        return lines[0].strip() if lines else result

    except Exception as e:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ–‡è„ˆãªã—ã§ç¿»è¨³
        return translate_with_gemini(text, speaker)


def review_translations_with_context(segments: list, use_gemini: bool = True) -> list:
    """
    å…¨ä½“ã®æ–‡è„ˆã‚’è¦‹ã¦ç¿»è¨³ã‚’è¦‹ç›´ã—ãƒ»ä¿®æ­£

    ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ã¯ä¼šè©±ã®æµã‚ŒãŒã‚ã‚‹ãŸã‚ã€å€‹åˆ¥ã®ç¿»è¨³ã ã‘ã§ã¯
    æ–‡è„ˆã«åˆã‚ãªã„ä¸è‡ªç„¶ãªç¿»è¨³ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
    å…¨ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä¸€è¦§ã—ã¦ã€æ–‡è„ˆã«åˆã‚ãªã„ç¿»è¨³ã‚’ä¿®æ­£ã™ã‚‹ã€‚

    Args:
        segments: ç¿»è¨³æ¸ˆã¿ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        use_gemini: Gemini APIã‚’ä½¿ç”¨ã™ã‚‹ã‹

    Returns:
        æ–‡è„ˆä¿®æ­£æ¸ˆã¿ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    """
    if not use_gemini or not segments:
        return segments

    api_key = get_gemini_api_key()
    if not api_key:
        print("   âš ï¸ GEMINI_API_KEYæœªè¨­å®šã€æ–‡è„ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        return segments

    print("   ğŸ” å…¨ä½“ã®æ–‡è„ˆã‚’åˆ†æä¸­...")

    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')

        # ä¼šè©±å…¨ä½“ã‚’æ§‹ç¯‰
        conversation_lines = []
        for i, seg in enumerate(segments):
            speaker = seg.get('speaker', '???')
            speaker = speaker.capitalize() if speaker else '???'
            text_ja = seg.get('text_ja', seg.get('text', ''))
            text_ko = seg.get('text', '')
            time_str = sec_to_time(seg['start'])
            conversation_lines.append(f"{i+1}. [{time_str}] {speaker}: {text_ja} (éŸ“å›½èª: {text_ko})")

        conversation_text = "\n".join(conversation_lines)

        prompt = f"""ã‚ãªãŸã¯League of Legends (LoL)ã®ãƒ—ãƒ­ãƒãƒ¼ãƒ ã®ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ç¿»è¨³ã‚’æ ¡æ­£ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

## ã‚¿ã‚¹ã‚¯
ä»¥ä¸‹ã®ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ç¿»è¨³ã‚’å…¨ä½“ã®æ–‡è„ˆã‚’è¦‹ã¦ç¢ºèªã—ã€**ãƒãƒ¼ãƒ ã®ä¼šè©±ãƒ»å—ã‘ç­”ãˆã®æµã‚Œ**ã‚’é‡è¦–ã—ã¦ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

## æœ€é‡è¦ï¼šãƒãƒ¼ãƒ ã®å—ã‘ç­”ãˆã‚’ç¿»è¨³ã™ã‚‹
ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ã¯**è¤‡æ•°äººã®ä¼šè©±**ã§ã™ã€‚èª°ã‹ã®ç™ºè¨€ã«å¯¾ã—ã¦åˆ¥ã®ãƒ¡ãƒ³ãƒãƒ¼ãŒå¿œç­”ã—ã¾ã™ã€‚
- è³ªå• â†’ å›ç­”ã®æµã‚Œã‚’è‡ªç„¶ã«
- ææ¡ˆ â†’ è³›æˆ/åå¯¾ã®æµã‚Œã‚’è‡ªç„¶ã«
- æŒ‡ç¤º â†’ äº†è§£/å®Ÿè¡Œã®æµã‚Œã‚’è‡ªç„¶ã«
- çŠ¶æ³å ±å‘Š â†’ ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®æµã‚Œã‚’è‡ªç„¶ã«

ä¾‹ï¼š
- Aã€Œè¡Œãï¼Ÿã€â†’ Bã€Œè¡Œã“ã†ï¼ã€ï¼ˆå—ã‘ç­”ãˆï¼‰
- Aã€Œãƒãƒ­ãƒ³è¦‹ã¦ã€â†’ Bã€ŒãŠã£ã‘ã€â†’ Cã€Œä¿ºãŒã‚¿ãƒ³ã‚¯ã™ã‚‹ã€ï¼ˆè¤‡æ•°äººã®ä¼šè©±ï¼‰
- Aã€Œãƒ•ãƒ©ãƒƒã‚·ãƒ¥ãªã„ã€â†’ Bã€Œã˜ã‚ƒã‚ç‹™ãŠã†ã€ï¼ˆæƒ…å ±å…±æœ‰â†’åˆ¤æ–­ï¼‰

## ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹ã®ç‰¹å¾´
- è©¦åˆä¸­ã®çŸ­ã„ã‚³ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆï¼ˆã‚ªãƒªã‚¢ãƒŠï¼ã‚°ã‚¦ã‚§ãƒ³ï¼ãªã©ï¼‰
- çŠ¶æ³å ±å‘Šï¼ˆæ¥ã¦ã‚‹ã€ã„ãªã„ã€TPä½¿ã£ãŸï¼‰
- æŒ‡ç¤ºï¼ˆä¸‹ãŒã£ã¦ã€åˆæµã—ã¦ã€è¡Œã“ã†ï¼‰
- ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒŠã‚¤ã‚¹ï¼ã‚„ã°ï¼ã†ã‚ãƒ¼ï¼‰
- é›†å›£æˆ¦ä¸­ã®å«ã³å£°ã‚„ç››ã‚Šä¸ŠãŒã‚Š
- **ç›¸æ‰‹ã®ç™ºè¨€ã¸ã®è¿”äº‹**ï¼ˆãŠã£ã‘ã€äº†è§£ã€åˆ†ã‹ã£ãŸã€ã„ã„ã­ï¼‰

## ã‚ˆãã‚ã‚‹èª¤è¨³ãƒ‘ã‚¿ãƒ¼ãƒ³
1. ã€Œãªã©è¨€ã£ã¦ã€ã€Œã¨ã—ã¦ã€ãªã©ã€æ–‡æœ«ãŒä¸è‡ªç„¶
2. è©¦åˆä¸­ã®ã‚³ãƒ¼ãƒ«ã«åˆã‚ãªã„ä¸å¯§ãªæ–‡ï¼ˆã€Œç§ãŒã€â†’ã€Œä¿ºãŒã€ãªã©ï¼‰
3. åŒã˜å†…å®¹ã®ç¹°ã‚Šè¿”ã—ã‚³ãƒ¼ãƒ«ãŒç•°ãªã‚‹æ„å‘³ã«ç¿»è¨³ã•ã‚Œã‚‹
4. ä¼šè©±ã®æµã‚ŒãŒé€”åˆ‡ã‚Œã‚‹ç¿»è¨³ï¼ˆå‰ã®ç™ºè¨€ã‚’å—ã‘ã¦ã„ãªã„ï¼‰

## çµ¶å¯¾ã«å®ˆã‚‹ãƒ«ãƒ¼ãƒ«
- ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³åï¼ˆã‚ªãƒªã‚¢ãƒŠã€ã‚°ã‚¦ã‚§ãƒ³ã€ã‚°ãƒ©ã‚¬ã‚¹ã€ãƒ¤ã‚¹ã‚ªã€ãƒ¨ãƒãªã©ï¼‰ã¯çµ¶å¯¾ã«å¤‰æ›´ç¦æ­¢
- ã€Œã‚ªãƒªã‚¢ãƒŠã€ã‚’ã€Œå³ã€ã«å¤‰ãˆã¦ã¯ã„ã‘ãªã„ï¼ˆã“ã‚Œã¯èª¤ã‚Šã§ã™ï¼‰
- ã€Œã‚ªãƒªã‚¢ãƒŠã€ã‚ªãƒªã‚¢ãƒŠã€ã€Œã‚ªãƒªã‚¢ãƒŠè¦‹ã¦ã€ãªã©ã¯ãã®ã¾ã¾ç¶­æŒã™ã‚‹ã“ã¨
- è©¦åˆä¸­ã¯æ•µãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³åã‚’å«ã¶ã®ãŒæ™®é€šãªã®ã§ã€ãƒãƒ£ãƒ³ãƒ”ã‚ªãƒ³åã¯ãã®ã¾ã¾ã«ã™ã‚‹

## ç¾åœ¨ã®ç¿»è¨³
{conversation_text}

## æŒ‡ç¤º
1. ä¸Šè¨˜ã®ç¿»è¨³ã‚’**ä¼šè©±ã®æµã‚Œ**ã¨ã—ã¦èª­ã‚€
2. èª°ã‹ã®ç™ºè¨€ã«å¯¾ã™ã‚‹å¿œç­”ã¨ã—ã¦ä¸è‡ªç„¶ãªç¿»è¨³ã‚’ç‰¹å®š
3. ãƒãƒ¼ãƒ ã®å—ã‘ç­”ãˆã¨ã—ã¦è‡ªç„¶ã«ãªã‚‹ã‚ˆã†ä¿®æ­£
4. ä¿®æ­£ãŒå¿…è¦ãªè¡Œã®ã¿ã€ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›

å‡ºåŠ›å½¢å¼ï¼ˆä¿®æ­£ã¾ãŸã¯å‰Šé™¤ãŒå¿…è¦ãªè¡Œã®ã¿ï¼‰:
```json
[
  {{"index": 1, "action": "fix", "original": "å…ƒã®ç¿»è¨³", "corrected": "ä¿®æ­£å¾Œã®ç¿»è¨³", "reason": "ä¿®æ­£ç†ç”±"}},
  {{"index": 2, "action": "remove", "original": "å…ƒã®ç¿»è¨³", "reason": "å‰Šé™¤ç†ç”±"}},
  ...
]
```

é‡è¦ï¼š
- action ã¯ "fix"ï¼ˆä¿®æ­£ï¼‰ã¾ãŸã¯ "remove"ï¼ˆå‰Šé™¤ï¼‰
- "fix" ã®å ´åˆã€correctedã«ã¯ç¿»è¨³ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å…¥ã‚Œã¦ãã ã•ã„ï¼ˆè©±è€…åã¯å«ã‚ãªã„ï¼‰
- **ä¼šè©±ã®å—ã‘ç­”ãˆã‚’æ„è­˜**ï¼šå‰ã®ç™ºè¨€ã‚’å—ã‘ãŸå¿œç­”ã¨ã—ã¦è‡ªç„¶ã‹ï¼Ÿ
- è³ªå•ã«ã¯å›ç­”ã€ææ¡ˆã«ã¯è³›å¦ã€å ±å‘Šã«ã¯ãƒªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã¯ãš
- ä¼šè©±ã®æµã‚Œã«åˆã‚ãªã„ä¸è‡ªç„¶ãªã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ "remove" ã§å‰Šé™¤æŒ‡å®š
- ä¾‹ï¼šé›†å›£æˆ¦ã®æœ€å¾Œã«çªç„¶ã€Œåˆ†ã‹ã‚‹ï¼Ÿã€ã€Œæœ¬å½“ã«ï¼Ÿã€ãªã©è„ˆçµ¡ã®ãªã„ç™ºè¨€
- ä¾‹ï¼šéŸ³å£°èªè­˜ã®èª¤ã‚Šã§æ„å‘³ä¸æ˜ãªæ–‡
- ä¿®æ­£ãƒ»å‰Šé™¤ä¸è¦ã®å ´åˆã¯ç©ºé…åˆ— `[]` ã‚’è¿”ã—ã¦ãã ã•ã„"""

        response = model.generate_content(prompt)
        result = response.text.strip()

        # JSONã‚’ãƒ‘ãƒ¼ã‚¹
        corrections = _parse_context_corrections(result)

        if not corrections:
            print("   âœ… æ–‡è„ˆä¸Šã®å•é¡Œãªã—")
            return segments

        # ä¿®æ­£ãƒ»å‰Šé™¤ã‚’é©ç”¨
        fix_count = sum(1 for c in corrections if c.get('action') == 'fix')
        remove_count = sum(1 for c in corrections if c.get('action') == 'remove')
        print(f"   ğŸ“ æ–‡è„ˆãƒ¬ãƒ“ãƒ¥ãƒ¼: ä¿®æ­£{fix_count}ä»¶, å‰Šé™¤{remove_count}ä»¶")

        for corr in corrections:
            idx = corr.get('index', 0) - 1  # 1-indexed to 0-indexed
            if 0 <= idx < len(segments):
                action = corr.get('action', 'fix')
                original = segments[idx].get('text_ja', '')
                reason = corr.get('reason', '')

                if action == 'remove':
                    # å‰Šé™¤ãƒãƒ¼ã‚¯ã‚’ä»˜ã‘ã‚‹
                    segments[idx]['removed'] = True
                    print(f"      [{idx+1}] âŒ å‰Šé™¤: {original}")
                    if reason:
                        print(f"          ç†ç”±: {reason}")
                else:
                    # ä¿®æ­£
                    corrected = corr.get('corrected', '')

                    # è©±è€…åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’é™¤å»ï¼ˆGeminiãŒå«ã‚ã¦ã—ã¾ã£ãŸå ´åˆï¼‰
                    if corrected:
                        corrected = re.sub(r'^[A-Za-z]+[ï¼š:]\s*', '', corrected)

                    if corrected and corrected != original:
                        segments[idx]['text_ja'] = corrected
                        segments[idx]['context_corrected'] = True
                        print(f"      [{idx+1}] {original} â†’ {corrected}")
                        if reason:
                            print(f"          ç†ç”±: {reason}")

        # å‰Šé™¤ãƒãƒ¼ã‚¯ã®ã¤ã„ãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é™¤å¤–
        segments = [s for s in segments if not s.get('removed', False)]

        return segments

    except Exception as e:
        print(f"   âš ï¸ æ–‡è„ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
        return segments


def _parse_context_corrections(text: str) -> list:
    """æ–‡è„ˆä¿®æ­£ã®JSONå¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹"""
    text = text.strip()

    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æŠ½å‡º
    if "```json" in text:
        start = text.index("```json") + 7
        end = text.index("```", start)
        text = text[start:end].strip()
    elif "```" in text:
        start = text.index("```") + 3
        end = text.index("```", start)
        text = text[start:end].strip()

    # é…åˆ—ã‚’æ¢ã™
    if "[" in text and "]" in text:
        start = text.index("[")
        end = text.rindex("]") + 1
        text = text[start:end]

    try:
        data = json.loads(text)
        if not isinstance(data, list):
            return []
        return data
    except json.JSONDecodeError:
        return []


def merge_conversation_segments(segments: list, gap_threshold: float = 1.0,
                                  same_speaker_gap: float = 2.0) -> list:
    """
    ä¼šè©±ã®æµã‚Œã«æ²¿ã£ã¦ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¸

    Args:
        segments: å…ƒã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        gap_threshold: è©±è€…ãŒå¤‰ã‚ã£ã¦ã‚‚çµ±åˆã™ã‚‹æœ€å¤§ã‚®ãƒ£ãƒƒãƒ—ï¼ˆç§’ï¼‰
        same_speaker_gap: åŒã˜è©±è€…ã®ç™ºè¨€ã‚’çµ±åˆã™ã‚‹æœ€å¤§ã‚®ãƒ£ãƒƒãƒ—ï¼ˆç§’ï¼‰

    Returns:
        ãƒãƒ¼ã‚¸ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
    """
    if not segments:
        return []

    merged = []
    current = {
        'start': segments[0]['start'],
        'end': segments[0]['end'],
        'texts': [segments[0].get('text_ja', segments[0]['text'])],
        'speakers': [segments[0]['speaker']],
        'scores': [segments[0]['score']],
    }

    for seg in segments[1:]:
        gap = seg['start'] - current['end']
        same_speaker = seg['speaker'] == current['speakers'][-1]

        # ãƒãƒ¼ã‚¸æ¡ä»¶:
        # 1. åŒã˜è©±è€…ã§ gap < same_speaker_gap
        # 2. é•ã†è©±è€…ã§ã‚‚ gap < gap_thresholdï¼ˆä¼šè©±ã®æµã‚Œã‚’ä¿ã¤ï¼‰
        should_merge = False

        if same_speaker and gap < same_speaker_gap:
            should_merge = True
        elif gap < gap_threshold and len(current['texts']) < 4:
            # çŸ­ã„ã‚®ãƒ£ãƒƒãƒ—ã§ã€ã¾ã çµ±åˆæ•°ãŒå°‘ãªã„å ´åˆ
            should_merge = True

        if should_merge:
            current['end'] = seg['end']
            current['texts'].append(seg.get('text_ja', seg['text']))
            current['speakers'].append(seg['speaker'])
            current['scores'].append(seg['score'])
        else:
            # ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç¢ºå®š
            merged.append(_finalize_merged_segment(current))
            # æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–‹å§‹
            current = {
                'start': seg['start'],
                'end': seg['end'],
                'texts': [seg.get('text_ja', seg['text'])],
                'speakers': [seg['speaker']],
                'scores': [seg['score']],
            }

    # æœ€å¾Œã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
    merged.append(_finalize_merged_segment(current))

    return merged


def _finalize_merged_segment(current: dict) -> dict:
    """ãƒãƒ¼ã‚¸ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ç¢ºå®š"""
    from collections import Counter

    # æœ€ã‚‚å¤šã„è©±è€…ã‚’é¸æŠ
    speaker_counts = Counter(current['speakers'])
    main_speaker = speaker_counts.most_common(1)[0][0]

    # ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
    combined_text = ' '.join(current['texts'])

    # å¹³å‡ã‚¹ã‚³ã‚¢
    avg_score = sum(current['scores']) / len(current['scores'])

    # è©±è€…ãŒè¤‡æ•°ã„ã‚‹å ´åˆã®ãƒãƒ¼ã‚¯
    unique_speakers = list(set(current['speakers']))
    if len(unique_speakers) > 1:
        speaker_display = f"{main_speaker}ã‚‰"
    else:
        speaker_display = main_speaker

    return {
        'start': current['start'],
        'end': current['end'],
        'text_ja': combined_text,
        'speaker': speaker_display,
        'score': avg_score,
        'segment_count': len(current['texts']),
    }


# ãƒ­ãƒ¼ãƒ«åˆ¥ã®ç‰¹å¾´çš„ãªã‚³ãƒ¼ãƒ«ï¼ˆéŸ“å›½èªï¼‰
ROLE_KEYWORDS = {
    'jungle': ['ê°±', 'ì •ê¸€', 'ì¹´ìš´í„°', 'ì˜¤ë¸Œì íŠ¸', 'ë“œë˜ê³¤', 'ë°”ë¡ ', 'í—¤ëŸ´ë“œ', 'í¬ë©'],
    'support': ['ì™€ë“œ', 'í•‘í¬', 'ì‹œì•¼', 'ë¡œë°', 'í', 'ì‹¤ë“œ'],
    'mid': ['ë¯¸ë“œ', 'ë¡œë°', 'ì†”í‚¬'],
    'top': ['íƒ‘', 'í…”', 'tp', 'ìŠ¤í”Œë¦¿'],
    'adc': ['ë”œ', 'í¬ì§€ì…˜', 'cs'],
}

# ãƒãƒ¼ãƒ ã®ãƒ­ãƒ¼ãƒ«æƒ…å ±
TEAM_ROLES = {
    'T1': {'Doran': 'top', 'Oner': 'jungle', 'Faker': 'mid', 'Peyz': 'adc', 'Keria': 'support'},
    'GenG': {'Kiin': 'top', 'Canyon': 'jungle', 'Chovy': 'mid', 'Ruler': 'adc', 'Lehends': 'support'},
    'HLE': {'Zeus': 'top', 'Peanut': 'jungle', 'Zeka': 'mid', 'Viper': 'adc', 'Delight': 'support'},
}


def identify_speaker_for_segment(waveform, sr: int, start: float, end: float,
                                  encoder, embeddings: dict, text: str = "", team: str = None) -> tuple:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©±è€…ã‚’è­˜åˆ¥ï¼ˆæ”¹å–„ç‰ˆï¼‰"""
    start_sample = int(start * sr)
    end_sample = int(end * sr)

    if end_sample - start_sample < sr:
        center = (start_sample + end_sample) // 2
        start_sample = max(0, center - sr // 2)
        end_sample = min(waveform.shape[1], center + sr // 2)

    segment_audio = waveform[:, start_sample:end_sample]

    if segment_audio.shape[1] < sr // 2:
        return None, 0.0, None

    emb = encoder.encode_batch(segment_audio).squeeze().numpy()

    # å…¨é¸æ‰‹ã®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    scores = {}
    for name, ref_emb in embeddings.items():
        sim = np.dot(emb, ref_emb) / (np.linalg.norm(emb) * np.linalg.norm(ref_emb))
        scores[name] = sim

    # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    best_name, best_score = sorted_scores[0]
    second_name, second_score = sorted_scores[1] if len(sorted_scores) > 1 else (None, 0)

    # åƒ…å·®åˆ¤å®šï¼ˆ0.08ä»¥å†…ï¼‰
    confidence = 'high' if best_score > 0.5 else 'medium' if best_score > 0.4 else 'low'

    if best_score - second_score < 0.08:
        confidence = 'uncertain'

        # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒ¼ãƒ«æ¨æ¸¬ã§è£œæ­£
        if text and team and team in TEAM_ROLES:
            text_lower = text.lower()
            role_scores = {name: 0 for name in embeddings.keys()}

            for role, keywords in ROLE_KEYWORDS.items():
                for kw in keywords:
                    if kw in text_lower:
                        # ã“ã®ãƒ­ãƒ¼ãƒ«ã®é¸æ‰‹ã«ãƒœãƒ¼ãƒŠã‚¹
                        for player, player_role in TEAM_ROLES.get(team, {}).items():
                            if player_role == role and player.lower() in embeddings:
                                role_scores[player.lower()] += 0.05

            # ãƒœãƒ¼ãƒŠã‚¹ã‚’é©ç”¨
            for name in [best_name, second_name]:
                if name and name.lower() in role_scores:
                    if role_scores[name.lower()] > 0:
                        scores[name] += role_scores[name.lower()]

            # å†è¨ˆç®—
            sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)
            best_name, best_score = sorted_scores[0]

    return best_name, float(best_score), confidence


def update_speaker_embedding(speaker: str, audio_path: str, start: float, end: float,
                             encoder, weight_new: float = 0.3):
    """ç¢ºèªã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ›´æ–°"""
    waveform, sr = torchaudio.load(audio_path)
    start_sample = int(start * sr)
    end_sample = int(end * sr)

    # æœ€ä½1ç§’ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ
    if end_sample - start_sample < sr:
        return False, "ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒçŸ­ã™ãã¾ã™"

    segment = waveform[:, start_sample:end_sample]
    new_emb = encoder.encode_batch(segment).squeeze().numpy()

    # æ—¢å­˜ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’èª­ã¿è¾¼ã¿
    emb_path = os.path.join(EMBEDDINGS_DIR, f'{speaker.lower()}.npy')
    if os.path.exists(emb_path):
        existing = np.load(emb_path)
        # é‡ã¿ä»˜ã‘çµåˆ
        combined = (1 - weight_new) * existing + weight_new * new_emb
        combined = combined / np.linalg.norm(combined) * np.linalg.norm(existing)
    else:
        combined = new_emb

    np.save(emb_path, combined)
    return True, f"{speaker}ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ›´æ–°ã—ã¾ã—ãŸ"


def add_subtitles(video_path: str, segments: list, output_path: str, style: str = "pro"):
    """å‹•ç”»ã«å­—å¹•ã‚’è¿½åŠ 

    Args:
        video_path: å…¥åŠ›å‹•ç”»ãƒ‘ã‚¹
        segments: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ
        output_path: å‡ºåŠ›ãƒ‘ã‚¹
        style: å­—å¹•ã‚¹ã‚¿ã‚¤ãƒ« ("pro" = ãƒ—ãƒ­é¢¨, "simple" = ã‚·ãƒ³ãƒ—ãƒ«)
    """
    video = VideoFileClip(video_path)

    # è©±è€…ã”ã¨ã®è‰²ï¼ˆè¦‹ã‚„ã™ã„è‰²ã‚’é¸æŠï¼‰
    SPEAKER_COLORS = {
        # T1
        'faker': '#FF6B6B',    # èµ¤ï¼ˆã‚³ãƒ¼ãƒ©ãƒ«ãƒ¬ãƒƒãƒ‰ï¼‰
        'oner': '#4ECDC4',     # ã‚¿ãƒ¼ã‚³ã‚¤ã‚º
        'doran': '#FFE66D',    # é»„è‰²
        'peyz': '#95E1D3',     # ãƒŸãƒ³ãƒˆã‚°ãƒªãƒ¼ãƒ³
        'keria': '#DDA0DD',    # ãƒ—ãƒ©ãƒ ï¼ˆç´«ï¼‰
        # GenG
        'chovy': '#FF6B6B',
        'canyon': '#4ECDC4',
        'kiin': '#FFE66D',
        'ruler': '#95E1D3',
        'lehends': '#DDA0DD',
        # HLE
        'zeus': '#FF6B6B',
        'peanut': '#4ECDC4',
        'zeka': '#FFE66D',
        'viper': '#95E1D3',
        'delight': '#DDA0DD',
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        '???': '#FFFFFF',
    }

    text_clips = []

    if style == "pro":
        # ãƒ—ãƒ­é¢¨ã‚¹ã‚¿ã‚¤ãƒ«: #PlayerName: ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè©±è€…åˆ¥è‰²åˆ†ã‘ï¼‰
        for seg in segments:
            if seg['end'] > video.duration:
                seg['end'] = video.duration
            if seg['start'] >= video.duration:
                continue

            speaker = seg['speaker'].capitalize() if seg['speaker'] else '???'
            # ã€Œã‚‰ã€ã‚’é™¤å»ï¼ˆè¤‡æ•°è©±è€…ãƒãƒ¼ã‚¯ï¼‰
            speaker_clean = speaker.replace('ã‚‰', '')
            text = seg.get('text_ja', seg.get('text', ''))

            # è©±è€…ã®è‰²ã‚’å–å¾—
            speaker_lower = speaker_clean.lower()
            color = SPEAKER_COLORS.get(speaker_lower, '#FFD700')  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚´ãƒ¼ãƒ«ãƒ‰

            # ãƒ—ãƒ­é¢¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: #Speaker: ãƒ†ã‚­ã‚¹ãƒˆ
            display_text = f"#{speaker_clean}: {text}"

            # å­—å¹•ã‚¯ãƒªãƒƒãƒ—ä½œæˆï¼ˆè©±è€…åˆ¥è‰²ã€ã‚¢ã‚¦ãƒˆãƒ©ã‚¤ãƒ³ä»˜ãï¼‰
            txt = TextClip(
                text=display_text,
                font_size=28,
                font="/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W6.ttc",
                color=color,
                stroke_color="black",
                stroke_width=2,
                method="caption",
                size=(video.w - 60, None)
            )

            # ä½ç½®: ç”»é¢ä¸‹éƒ¨ä¸­å¤®
            y_pos = video.h - 80
            txt = txt.with_position(("center", y_pos)).with_start(seg['start']).with_end(seg['end'])
            text_clips.append(txt)

    else:
        # ã‚·ãƒ³ãƒ—ãƒ«ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆå¾“æ¥ï¼‰
        for seg in segments:
            if seg['end'] > video.duration:
                seg['end'] = video.duration
            if seg['start'] >= video.duration:
                continue

            color = "lime" if seg['score'] > 0.5 else "yellow" if seg['score'] > 0.4 else "red"
            speaker = seg['speaker'].capitalize() if seg['speaker'] else '???'
            text = seg.get('text_ja', seg.get('text', ''))
            display_text = f"[{speaker}] {text}"

            txt = TextClip(
                text=display_text,
                font_size=22,
                font="/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒè§’ã‚´ã‚·ãƒƒã‚¯ W6.ttc",
                color=color,
                bg_color="black",
                margin=(8, 4),
                method="caption",
                size=(video.w - 40, None)
            )
            txt = txt.with_position(("center", video.h - 70)).with_start(seg['start']).with_end(seg['end'])
            text_clips.append(txt)

    final = CompositeVideoClip([video] + text_clips)
    final.write_videofile(output_path, codec="libx264", audio_codec="aac", logger=None)

    video.close()
    final.close()


def auto_learning(segments: list, audio_path: str):
    """é«˜ã‚¹ã‚³ã‚¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•å­¦ç¿’"""
    print("\n" + "="*50)
    print("ğŸ¤– è‡ªå‹•å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰")
    print("="*50)

    encoder = EncoderClassifier.from_hparams(
        source='speechbrain/spkrec-ecapa-voxceleb',
        savedir=CACHE_DIR
    )

    learned_count = 0

    for seg in segments:
        speaker = seg['speaker']
        score = seg['score']

        if not speaker:
            continue

        # ã‚¹ã‚³ã‚¢ã«å¿œã˜ãŸå­¦ç¿’
        if score > 0.6:
            # é«˜ã‚¹ã‚³ã‚¢: è‡ªå‹•å­¦ç¿’ï¼ˆ10%ã®é‡ã¿ï¼‰
            success, msg = update_speaker_embedding(
                speaker, audio_path, seg['start'], seg['end'], encoder, weight_new=0.1
            )
            if success:
                learned_count += 1
                print(f"  ğŸŸ¢ {speaker.capitalize()}: {seg.get('text_ja', seg['text'])[:30]}... (è‡ªå‹•å­¦ç¿’)")
        elif score > 0.5:
            # ä¸­é«˜ã‚¹ã‚³ã‚¢: è»½ã„å­¦ç¿’ï¼ˆ5%ã®é‡ã¿ï¼‰
            success, msg = update_speaker_embedding(
                speaker, audio_path, seg['start'], seg['end'], encoder, weight_new=0.05
            )
            if success:
                learned_count += 1
                print(f"  ğŸŸ¡ {speaker.capitalize()}: {seg.get('text_ja', seg['text'])[:30]}... (è»½å­¦ç¿’)")

    print(f"\nâœ… è‡ªå‹•å­¦ç¿’å®Œäº†: {learned_count}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§æ›´æ–°")
    return learned_count


def learn_lol_term(wrong: str, correct: str):
    """LoLç”¨èªã‚’ã‚«ã‚¹ã‚¿ãƒ è¾æ›¸ã«è¿½åŠ """
    custom = load_custom_dict()
    custom['terms'][wrong] = correct
    custom['learned_count'] = custom.get('learned_count', 0) + 1
    save_custom_dict(custom)
    print(f"   ğŸ“š LoLç”¨èªå­¦ç¿’: {wrong} â†’ {correct}")


def interactive_learning(segments: list, audio_path: str, team: str, db: dict):
    """å¯¾è©±çš„ã«çµæœã‚’ç¢ºèªãƒ»ä¿®æ­£ã—ã¦å­¦ç¿’"""
    print("\n" + "="*50)
    print("ğŸ“ å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰")
    print("="*50)
    print("å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®è©±è€…ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    print("Enter: æ­£ã—ã„ / é¸æ‰‹åã‚’å…¥åŠ›: ä¿®æ­£ / s: ã‚¹ã‚­ãƒƒãƒ— / q: çµ‚äº†")
    print("t: LoLç”¨èªä¿®æ­£ãƒ¢ãƒ¼ãƒ‰ï¼ˆèª¤è¨³ã‚’ä¿®æ­£ã—ã¦å­¦ç¿’ï¼‰")
    print()

    # ãƒãƒ¼ãƒ ã®é¸æ‰‹ãƒªã‚¹ãƒˆ
    if team and team in db['teams']:
        team_players = db['teams'][team]['players']
        print(f"ã€{team}ã®é¸æ‰‹ã€‘: {', '.join(team_players)}")
    print()

    encoder = EncoderClassifier.from_hparams(
        source='speechbrain/spkrec-ecapa-voxceleb',
        savedir=CACHE_DIR
    )

    learned_count = 0

    for i, seg in enumerate(segments):
        if seg['score'] < 0.3:  # ä½ã‚¹ã‚³ã‚¢ã¯ç¢ºèª
            needs_confirm = True
        elif seg['score'] > 0.6:  # é«˜ã‚¹ã‚³ã‚¢ã¯è‡ªå‹•æ‰¿èª
            needs_confirm = False
        else:
            needs_confirm = True

        speaker = seg['speaker'].capitalize() if seg['speaker'] else '???'
        conf = "ğŸŸ¢" if seg['score'] > 0.5 else "ğŸŸ¡" if seg['score'] > 0.4 else "ğŸ”´"

        print(f"[{i+1}/{len(segments)}] {sec_to_time(seg['start'])} {conf}")
        print(f"  è©±è€…: {speaker} ({seg['score']:.2f})")
        print(f"  å†…å®¹: {seg.get('text_ja', seg['text'])}")

        if not needs_confirm:
            print(f"  â†’ è‡ªå‹•æ‰¿èªï¼ˆé«˜ã‚¹ã‚³ã‚¢ï¼‰")
            # é«˜ã‚¹ã‚³ã‚¢ã®ã‚‚ã®ã¯è‡ªå‹•ã§å­¦ç¿’
            if seg['speaker'] and seg['score'] > 0.6:
                success, msg = update_speaker_embedding(
                    seg['speaker'], audio_path, seg['start'], seg['end'], encoder, weight_new=0.1
                )
                if success:
                    learned_count += 1
            print()
            continue

        response = input(f"  â†’ ç¢ºèª (Enter=OK / é¸æ‰‹å / s=skip / t=ç”¨èªä¿®æ­£ / q=quit): ").strip().lower()

        if response == 'q':
            break
        elif response == 's':
            print("  â†’ ã‚¹ã‚­ãƒƒãƒ—")
        elif response == 't':
            # LoLç”¨èªä¿®æ­£ãƒ¢ãƒ¼ãƒ‰
            wrong = input("  â†’ èª¤ã£ãŸå˜èª: ").strip()
            correct = input("  â†’ æ­£ã—ã„å˜èª: ").strip()
            if wrong and correct:
                learn_lol_term(wrong, correct)
        elif response == '':
            # ç¢ºèªOKã€å­¦ç¿’
            if seg['speaker']:
                success, msg = update_speaker_embedding(
                    seg['speaker'], audio_path, seg['start'], seg['end'], encoder, weight_new=0.2
                )
                if success:
                    learned_count += 1
                    print(f"  â†’ {msg}")
        else:
            # ä¿®æ­£ã•ã‚ŒãŸå ´åˆ
            corrected_speaker = response
            success, msg = update_speaker_embedding(
                corrected_speaker, audio_path, seg['start'], seg['end'], encoder, weight_new=0.3
            )
            if success:
                learned_count += 1
                print(f"  â†’ {msg}")
            else:
                print(f"  â†’ ã‚¨ãƒ©ãƒ¼: {msg}")

        print()

    print(f"\nâœ… å­¦ç¿’å®Œäº†: {learned_count}å€‹ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã§æ›´æ–°ã—ã¾ã—ãŸ")
    return learned_count


def detect_teamvoice_highlights(url: str, min_score: float = 0.3) -> list:
    """é•·ã„å‹•ç”»ã‹ã‚‰ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹éƒ¨åˆ†ã‚’è‡ªå‹•æ¤œå‡º"""
    api_key = get_gemini_api_key()
    if not api_key:
        print("âŒ GEMINI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        print("   export GEMINI_API_KEY='your-api-key'")
        return []

    print("=== ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ ===")
    print(f"URL: {url}")

    # 1. å‹•ç”»ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…¨ä½“ï¼‰
    print("\n1. å‹•ç”»ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    clip_path, title = download_clip(url)
    print(f"   å®Œäº†: {title}")

    # 2. éŸ³å£°æŠ½å‡º
    print("\n2. éŸ³å£°æŠ½å‡ºä¸­...")
    audio_path = extract_audio(clip_path)
    print("   å®Œäº†")

    # 3. éŸ³å£°èªè­˜
    print("\n3. éŸ³å£°èªè­˜ä¸­ï¼ˆWhisperï¼‰...")
    whisper_segments = transcribe_audio(audio_path, language="ko")
    print(f"   {len(whisper_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆèªè­˜")

    # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
    lines = []
    for seg in whisper_segments:
        minutes = int(seg['start'] // 60)
        secs = int(seg['start'] % 60)
        lines.append(f"[{minutes:02d}:{secs:02d}] {seg['text']}")
    transcript_text = "\n".join(lines)

    # 4. Geminiã§ãƒã‚¤ãƒ©ã‚¤ãƒˆæ¤œå‡º
    print("\n4. ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹æ¤œå‡ºä¸­ï¼ˆGemini APIï¼‰...")

    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.0-flash')

        # ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒé•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²
        max_length = 15000
        if len(transcript_text) > max_length:
            transcript_text = transcript_text[:max_length] + "\n...(truncated)"

        prompt = TEAMVOICE_DETECT_PROMPT.format(transcript=transcript_text)
        response = model.generate_content(prompt)
        result = response.text

        # JSONãƒ‘ãƒ¼ã‚¹
        highlights = parse_highlight_json(result)

        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        highlights = [h for h in highlights if h['score'] >= min_score]
        highlights = sorted(highlights, key=lambda h: h['start'])

        print(f"\n   æ¤œå‡ºã•ã‚ŒãŸãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹: {len(highlights)}ä»¶")
        for i, h in enumerate(highlights):
            start = f"{int(h['start'] // 60):02d}:{int(h['start'] % 60):02d}"
            end = f"{int(h['end'] // 60):02d}:{int(h['end'] % 60):02d}"
            print(f"   {i+1}. [{start} - {end}] {h['title']} (ã‚¹ã‚³ã‚¢: {h['score']:.2f})")

        return highlights

    except Exception as e:
        print(f"âŒ Gemini API ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def parse_highlight_json(text: str) -> list:
    """Geminiã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ãƒã‚¤ãƒ©ã‚¤ãƒˆJSONã‚’ãƒ‘ãƒ¼ã‚¹"""
    text = text.strip()

    # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯å†…ã®JSONã‚’æŠ½å‡º
    if "```json" in text:
        start = text.index("```json") + 7
        end = text.index("```", start)
        text = text[start:end].strip()
    elif "```" in text:
        start = text.index("```") + 3
        end = text.index("```", start)
        text = text[start:end].strip()

    # é…åˆ—ã‚’æ¢ã™
    if "[" in text and "]" in text:
        start = text.index("[")
        end = text.rindex("]") + 1
        text = text[start:end]

    try:
        data = json.loads(text)
        if not isinstance(data, list):
            return []

        highlights = []
        for h in data:
            try:
                highlights.append({
                    'start': float(h.get('start', 0)),
                    'end': float(h.get('end', 0)),
                    'title': h.get('title', 'Unknown'),
                    'description': h.get('description', ''),
                    'type': h.get('type', 'other'),
                    'score': float(h.get('score', 0.5)),
                })
            except (ValueError, KeyError):
                continue

        return highlights

    except json.JSONDecodeError:
        return []


def create_clip_with_speaker(url: str, start_time: str, end_time: str,
                             team: str = None, output_name: str = None,
                             auto_detect_team: bool = True,
                             learn_mode: bool = False,
                             auto_learn_mode: bool = False,
                             use_gemini: bool = False,
                             use_papago: bool = False,
                             separate_audio: bool = False,
                             whisper_model: str = "base",
                             merge_mode: bool = False,
                             subtitle_style: str = "pro") -> str:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°

    Args:
        url: YouTube URL
        start_time: é–‹å§‹æ™‚é–“
        end_time: çµ‚äº†æ™‚é–“
        team: ãƒãƒ¼ãƒ æŒ‡å®š
        output_name: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
        auto_detect_team: ãƒãƒ¼ãƒ è‡ªå‹•æ¤œå‡º
        learn_mode: å¯¾è©±çš„å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
        auto_learn_mode: è‡ªå‹•å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
        use_gemini: Geminiç¿»è¨³ä½¿ç”¨
        use_papago: Papagoç¿»è¨³ä½¿ç”¨ï¼ˆéŸ“æ—¥ç‰¹åŒ–ã€æ¨å¥¨ï¼‰
        separate_audio: éŸ³å£°åˆ†é›¢ï¼ˆDemucsï¼‰ä½¿ç”¨
        whisper_model: Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º
    """
    start_sec = time_to_sec(start_time)
    end_sec = time_to_sec(end_time)
    duration = end_sec - start_sec

    print(f"=== åˆ‡ã‚ŠæŠœãå‹•ç”»ä½œæˆ ===")
    print(f"URL: {url}")
    print(f"åŒºé–“: {start_time} - {end_time} ({duration}ç§’)")

    # è¨­å®šè¡¨ç¤º
    settings = []
    if use_papago:
        settings.append("ğŸ‡°ğŸ‡· Papagoç¿»è¨³")
    elif use_gemini:
        settings.append("ğŸ¤– Geminiç¿»è¨³")
    else:
        settings.append("ğŸ“ googletrans")
    if separate_audio:
        settings.append("ğŸµ éŸ³å£°åˆ†é›¢")
    if whisper_model != "base":
        settings.append(f"ğŸ¤ Whisper-{whisper_model}")
    print(f"è¨­å®š: {' | '.join(settings)}")

    # LoLè¾æ›¸ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    base_corrections = create_correction_dict()
    custom_dict = load_custom_dict()
    print(f"ğŸ“š LoLè¾æ›¸: åŸºæœ¬{len(base_corrections)}ä»¶ + ã‚«ã‚¹ã‚¿ãƒ {len(custom_dict.get('terms', {}))}ä»¶")

    db = load_database()

    # 1. æŒ‡å®šåŒºé–“ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    print("\n1. æŒ‡å®šåŒºé–“ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    clip_path, title = download_clip(url, start_sec, end_sec)
    print(f"   å®Œäº†: {title}")

    # 2. ãƒãƒ¼ãƒ æ¤œå‡º
    detected_team = team
    if not team and auto_detect_team:
        print("\n2. ãƒãƒ¼ãƒ è‡ªå‹•æ¤œå‡ºä¸­...")
        detected_team = detect_team_from_title(title)
        if detected_team:
            print(f"   æ¤œå‡º: {detected_team}")
        else:
            print("   æ¤œå‡ºã§ããšã€å…¨é¸æ‰‹ã§è­˜åˆ¥")
    elif team:
        print(f"\n2. ãƒãƒ¼ãƒ æŒ‡å®š: {team}")

    # 3. éŸ³å£°æŠ½å‡º
    print("\n3. éŸ³å£°æŠ½å‡ºä¸­...")
    audio_path = extract_audio(clip_path)
    print("   å®Œäº†")

    # 3.5. éŸ³å£°åˆ†é›¢ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    if separate_audio:
        audio_path = separate_vocals(audio_path)

    # 4. éŸ³å£°èªè­˜
    print("\n4. éŸ³å£°èªè­˜ä¸­ï¼ˆWhisperï¼‰...")
    whisper_segments = transcribe_audio(audio_path, language="ko", model_size=whisper_model)
    print(f"   {len(whisper_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆèªè­˜")

    # 5. è©±è€…è­˜åˆ¥
    print("\n5. è©±è€…è­˜åˆ¥ä¸­...")
    embeddings = load_speaker_embeddings(detected_team, db)
    print(f"   {len(embeddings)}äººã®é¸æ‰‹ãƒ‡ãƒ¼ã‚¿ã§è­˜åˆ¥")

    encoder = EncoderClassifier.from_hparams(
        source='speechbrain/spkrec-ecapa-voxceleb',
        savedir=CACHE_DIR
    )
    waveform, sr = torchaudio.load(audio_path)

    segments_with_speaker = []
    for seg in whisper_segments:
        speaker, score, confidence = identify_speaker_for_segment(
            waveform, sr, seg['start'], seg['end'], encoder, embeddings,
            text=seg['text'], team=detected_team
        )
        segments_with_speaker.append({
            'start': seg['start'],
            'end': seg['end'],
            'text': seg['text'].strip(),
            'speaker': speaker,
            'score': score,
            'confidence': confidence
        })

    # 6. æ—¥æœ¬èªç¿»è¨³ + LoLç”¨èªä¿®æ­£ï¼ˆæ–‡è„ˆä»˜ãï¼‰
    print("\n6. æ—¥æœ¬èªç¿»è¨³ + LoLç”¨èªä¿®æ­£ä¸­...")
    corrections = get_merged_corrections()

    for i, seg in enumerate(segments_with_speaker):
        # å‰å¾Œã®æ–‡è„ˆã‚’å–å¾—
        context_before = segments_with_speaker[i-1]['text'] if i > 0 else ""
        context_after = segments_with_speaker[i+1]['text'] if i < len(segments_with_speaker)-1 else ""

        # ã¾ãšéŸ“å›½èªã®éŸ³å£°èªè­˜èª¤ã‚Šã‚’ä¿®æ­£
        text_asr_fixed = correct_korean_asr(seg['text'])
        # æ¬¡ã«LoLç”¨èªä¿®æ­£ã‚’é©ç”¨
        text_corrected = correct_text(text_asr_fixed, corrections)

        # ç¿»è¨³ï¼ˆæ–‡è„ˆä»˜ãï¼‰
        speaker_name = seg['speaker'].capitalize() if seg['speaker'] else ""
        translated = translate_to_japanese(
            text_corrected,
            speaker=speaker_name,
            use_gemini=use_gemini,
            use_papago=use_papago,
            context_before=context_before,
            context_after=context_after
        )
        # ç¿»è¨³å¾Œã®æ—¥æœ¬èªä¿®æ­£ï¼ˆPapagoç¿»è¨³ãƒŸã‚¹å¯¾å¿œï¼‰
        seg['text_ja'] = correct_japanese_post(translated)
    print("   å®Œäº†")

    # 7. å…¨ä½“ã®æ–‡è„ˆã‚’è¦‹ã¦ç¿»è¨³ã‚’è¦‹ç›´ã—ï¼ˆGeminiä½¿ç”¨æ™‚ã®ã¿ï¼‰
    if use_gemini or use_papago:
        print("\n7. å…¨ä½“ã®æ–‡è„ˆã§ç¿»è¨³ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­...")
        segments_with_speaker = review_translations_with_context(
            segments_with_speaker,
            use_gemini=True  # æ–‡è„ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å¸¸ã«Geminiã‚’ä½¿ç”¨
        )
        print("   å®Œäº†")

    # ä¼šè©±ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ¼ã‚¸
    if merge_mode:
        print("\n   ä¼šè©±ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰: ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆä¸­...")
        display_segments = merge_conversation_segments(segments_with_speaker)
        print(f"   {len(segments_with_speaker)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ â†’ {len(display_segments)}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«çµ±åˆ")
    else:
        display_segments = segments_with_speaker

    # çµæœè¡¨ç¤º
    mode_label = "ï¼ˆä¼šè©±ãƒ•ãƒ­ãƒ¼ï¼‰" if merge_mode else "ï¼ˆæ—¥æœ¬èªï¼‰"
    print(f"\n   === èªè­˜çµæœ{mode_label} ===")
    for seg in display_segments:
        if merge_mode:
            # ãƒãƒ¼ã‚¸ãƒ¢ãƒ¼ãƒ‰ã§ã¯çµ±åˆæ•°ã‚’è¡¨ç¤º
            count = seg.get('segment_count', 1)
            count_mark = f"[{count}]" if count > 1 else ""
            speaker = seg['speaker'].capitalize() if seg['speaker'] else '???'
            print(f"   {sec_to_time(seg['start'])}-{sec_to_time(seg['end'])} [{speaker}]{count_mark}: {seg['text_ja']}")
        else:
            confidence = seg.get('confidence', 'medium')
            if confidence == 'uncertain':
                conf_icon = "â“"
            elif seg['score'] > 0.5:
                conf_icon = "ğŸŸ¢"
            elif seg['score'] > 0.4:
                conf_icon = "ğŸŸ¡"
            else:
                conf_icon = "ğŸ”´"
            speaker = seg['speaker'].capitalize() if seg['speaker'] else '???'
            uncertain_mark = "?" if confidence == 'uncertain' else ""
            print(f"   {sec_to_time(seg['start'])} [{speaker}{uncertain_mark}] {conf_icon}: {seg['text_ja']}")

    # å±¥æ­´ä¿å­˜
    history = load_history()
    session = {
        'timestamp': datetime.now().isoformat(),
        'url': url,
        'title': title,
        'start': start_time,
        'end': end_time,
        'team': detected_team,
        'segments': segments_with_speaker,
        'audio_path': audio_path
    }
    history['sessions'].append(session)
    history['last_session'] = session
    save_history(history)

    # 8. å­—å¹•ä»˜ãå‹•ç”»ä½œæˆ
    print("\n8. å­—å¹•ä»˜ãå‹•ç”»ä½œæˆä¸­...")
    if output_name is None:
        team_suffix = f"_{detected_team}" if detected_team else ""
        output_name = f"clip_{start_time.replace(':', '')}_{end_time.replace(':', '')}{team_suffix}.mp4"
    output_path = os.path.join(OUTPUT_DIR, output_name)

    # ãƒãƒ¼ã‚¸ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯çµ±åˆã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨
    subtitle_segments = display_segments if merge_mode else segments_with_speaker
    add_subtitles(clip_path, subtitle_segments, output_path, style=subtitle_style)

    size = os.path.getsize(output_path) / 1024 / 1024
    print(f"\n=== å®Œäº† ===")
    print(f"å‡ºåŠ›: {output_path}")
    print(f"ã‚µã‚¤ã‚º: {size:.2f} MB")

    # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰
    if auto_learn_mode:
        auto_learning(segments_with_speaker, audio_path)
    elif learn_mode:
        interactive_learning(segments_with_speaker, audio_path, detected_team, db)

    # å“è³ªåˆ†æã¨è‡ªå‹•æ”¹å–„
    session_quality = analyze_session_quality(segments_with_speaker)
    if session_quality['suggest_improvement'] and detected_team and AUTO_COLLECT_AVAILABLE:
        if AUTO_IMPROVE_CONFIG['enabled']:
            # è‡ªå‹•æ”¹å–„ã‚’å®Ÿè¡Œ
            auto_improve_in_background(detected_team, session_quality)
        else:
            # æ‰‹å‹•æ”¹å–„ã‚’æ¨å¥¨
            print(f"\nğŸ’¡ è©±è€…è­˜åˆ¥ã®ç²¾åº¦ãŒä½ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒå¤šã„ã§ã™")
            print(f"   é«˜ä¿¡é ¼: {session_quality['high_count']}, "
                  f"ä¸­: {session_quality['medium_count']}, "
                  f"ä½: {session_quality['low_count']}, "
                  f"ä¸æ˜: {session_quality['unknown_count']}")
            print(f"   ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ”¹å–„ã‚’æ¨å¥¨:")
            print(f"   python clip_with_speaker.py --improve {detected_team}")

    return output_path


def correct_last_session():
    """ç›´è¿‘ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä¿®æ­£ã—ã¦å­¦ç¿’"""
    history = load_history()
    if 'last_session' not in history:
        print("å­¦ç¿’å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    session = history['last_session']
    print(f"\nç›´è¿‘ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³: {session['title']}")
    print(f"åŒºé–“: {session['start']} - {session['end']}")

    db = load_database()
    interactive_learning(
        session['segments'],
        session['audio_path'],
        session['team'],
        db
    )


def check_embedding_quality(team_name: str = None) -> dict:
    """ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯"""
    db = load_database()
    quality_report = {
        'high': [],
        'medium': [],
        'low': [],
        'new': [],
        'missing': [],
    }

    players = db.get('players', {})

    for name, info in players.items():
        # ãƒãƒ¼ãƒ æŒ‡å®šãŒã‚ã‚‹å ´åˆã¯ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if team_name and info.get('team') != team_name:
            continue

        acc = info.get('accuracy', {})
        level = acc.get('level', 'unknown')
        has_emb = info.get('has_embedding', False)

        if not has_emb:
            quality_report['missing'].append(name)
        elif level == 'high':
            quality_report['high'].append({'name': name, 'score': acc.get('avg', 0)})
        elif level == 'medium':
            quality_report['medium'].append({'name': name, 'score': acc.get('avg', 0)})
        elif level == 'low':
            quality_report['low'].append({'name': name, 'score': acc.get('avg', 0)})
        else:
            quality_report['new'].append(name)

    return quality_report


def improve_embeddings(team_name: str, auto_run: bool = False):
    """ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ”¹å–„"""
    print(f"\n{'='*60}")
    print(f"ğŸ”§ {team_name} ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ”¹å–„")
    print(f"{'='*60}")

    if not AUTO_COLLECT_AVAILABLE:
        print("âŒ auto_collect_voice ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        print("   pip install speechbrain torchaudio yt-dlp whisper ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return

    # å“è³ªãƒã‚§ãƒƒã‚¯
    quality = check_embedding_quality(team_name)

    print("\nğŸ“Š ç¾åœ¨ã®å“è³ª:")
    print(f"   ğŸŸ¢ é«˜å“è³ª: {len(quality['high'])}äºº")
    for p in quality['high']:
        print(f"      - {p['name']} (avg: {p['score']:.3f})")

    print(f"   ğŸŸ¡ ä¸­å“è³ª: {len(quality['medium'])}äºº")
    for p in quality['medium']:
        print(f"      - {p['name']} (avg: {p['score']:.3f})")

    print(f"   ğŸ”´ ä½å“è³ª: {len(quality['low'])}äºº")
    for p in quality['low']:
        print(f"      - {p['name']} (avg: {p['score']:.3f})")

    print(f"   âšª æ–°è¦: {len(quality['new'])}äºº")
    for p in quality['new']:
        print(f"      - {p}")

    print(f"   âŒ æœªç™»éŒ²: {len(quality['missing'])}äºº")
    for p in quality['missing']:
        print(f"      - {p}")

    # æ”¹å–„ãŒå¿…è¦ãªé¸æ‰‹ã‚’ç‰¹å®š
    needs_improvement = quality['low'] + quality['new']

    if not needs_improvement:
        print("\nâœ… å…¨ã¦ã®é¸æ‰‹ãŒä¸­ã€œé«˜å“è³ªã§ã™ã€‚æ”¹å–„ã¯ä¸è¦ã§ã™ã€‚")
        return

    print(f"\nğŸ¯ æ”¹å–„å¯¾è±¡: {len(needs_improvement)}äºº")

    if not auto_run:
        confirm = input("\nè‡ªå‹•åé›†ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n): ").strip().lower()
        if confirm != 'y':
            print("ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
            return

    # è©±è€…ç…§åˆæ–¹å¼ã§åé›†
    print("\nğŸš€ ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹å‹•ç”»ã‹ã‚‰è©±è€…ç…§åˆã§åé›†é–‹å§‹...")
    results = collect_with_diarization(
        team_name,
        max_videos=5,
        update_embedding=True,
        similarity_threshold=0.45
    )

    # çµæœã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ æ”¹å–„çµæœ")
    print(f"{'='*60}")

    improved = [p for p in results if results[p].get('count', 0) >= 3]
    print(f"   æ”¹å–„ã•ã‚ŒãŸé¸æ‰‹: {len(improved)}äºº")
    for p in improved:
        print(f"      - {p}: {results[p]['count']}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ (avg: {results[p]['avg_score']:.3f})")

    # å†åº¦å“è³ªãƒã‚§ãƒƒã‚¯
    print("\nğŸ“Š æ”¹å–„å¾Œã®å“è³ª:")
    new_quality = check_embedding_quality(team_name)
    print(f"   ğŸŸ¢ é«˜å“è³ª: {len(new_quality['high'])}äºº")
    print(f"   ğŸŸ¡ ä¸­å“è³ª: {len(new_quality['medium'])}äºº")
    print(f"   ğŸ”´ ä½å“è³ª: {len(new_quality['low'])}äºº")


def analyze_session_quality(segments: list) -> dict:
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å“è³ªã‚’åˆ†æã—ã€æ”¹å–„ãŒå¿…è¦ã‹åˆ¤æ–­"""
    total = len(segments)
    if total == 0:
        return {'quality': 'unknown', 'suggest_improvement': False}

    high = sum(1 for s in segments if s.get('confidence') == 'high')
    medium = sum(1 for s in segments if s.get('confidence') == 'medium')
    low = sum(1 for s in segments if s.get('confidence') == 'low')
    unknown = sum(1 for s in segments if (s.get('speaker') or '').startswith('???'))

    high_ratio = high / total
    low_ratio = (low + unknown) / total

    quality = 'high' if high_ratio >= 0.6 else 'medium' if high_ratio >= 0.3 else 'low'
    suggest = low_ratio >= 0.5  # 50%ä»¥ä¸ŠãŒä½ä¿¡é ¼ãªã‚‰æ”¹å–„ã‚’ææ¡ˆ

    return {
        'quality': quality,
        'high_count': high,
        'medium_count': medium,
        'low_count': low,
        'unknown_count': unknown,
        'total': total,
        'suggest_improvement': suggest,
    }


def load_auto_improve_log() -> dict:
    """è‡ªå‹•æ”¹å–„ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(AUTO_IMPROVE_LOG_PATH):
        with open(AUTO_IMPROVE_LOG_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {'last_improve': {}}


def save_auto_improve_log(log: dict):
    """è‡ªå‹•æ”¹å–„ãƒ­ã‚°ã‚’ä¿å­˜"""
    os.makedirs(os.path.dirname(AUTO_IMPROVE_LOG_PATH), exist_ok=True)
    with open(AUTO_IMPROVE_LOG_PATH, 'w', encoding='utf-8') as f:
        json.dump(log, f, ensure_ascii=False, indent=2)


def can_auto_improve(team_name: str) -> bool:
    """ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³æœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯"""
    log = load_auto_improve_log()
    last_time = log.get('last_improve', {}).get(team_name, 0)
    elapsed = datetime.now().timestamp() - last_time
    return elapsed >= AUTO_IMPROVE_CONFIG['cooldown']


def auto_improve_in_background(team_name: str, session_quality: dict):
    """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§è‡ªå‹•æ”¹å–„ã‚’å®Ÿè¡Œ"""
    if not AUTO_IMPROVE_CONFIG['enabled']:
        return

    if not AUTO_COLLECT_AVAILABLE:
        return

    if not can_auto_improve(team_name):
        print(f"   â³ {team_name}ã®è‡ªå‹•æ”¹å–„ã¯ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ä¸­ã§ã™")
        return

    # ãƒ­ã‚°æ›´æ–°
    log = load_auto_improve_log()
    if 'last_improve' not in log:
        log['last_improve'] = {}
    log['last_improve'][team_name] = datetime.now().timestamp()
    save_auto_improve_log(log)

    print(f"\nğŸ”„ è‡ªå‹•æ”¹å–„é–‹å§‹: {team_name}")
    print(f"   å“è³ª: é«˜{session_quality['high_count']}, "
          f"ä¸­{session_quality['medium_count']}, "
          f"ä½{session_quality['low_count']}, "
          f"ä¸æ˜{session_quality['unknown_count']}")

    # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æ”¹å–„ã‚’å®Ÿè¡Œ
    def run_improvement():
        try:
            # åé›†å®Ÿè¡Œï¼ˆæœ€å¤§3å‹•ç”»ï¼‰
            results = collect_with_diarization(team_name, max_videos=3)

            if results:
                print(f"\nâœ… {team_name}ã®è‡ªå‹•æ”¹å–„å®Œäº†")
                for player, data in results.items():
                    print(f"   {player}: {data['count']}ã‚»ã‚°ãƒ¡ãƒ³ãƒˆåé›†")
            else:
                print(f"\nâš ï¸ {team_name}ã®è‡ªå‹•æ”¹å–„: æ–°ã—ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãªã—")
        except Exception as e:
            print(f"\nâŒ è‡ªå‹•æ”¹å–„ã‚¨ãƒ©ãƒ¼: {e}")

    thread = threading.Thread(target=run_improvement, daemon=True)
    thread.start()
    print("   ğŸ’¡ ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§æ”¹å–„ä¸­... æ¬¡ã®å‡¦ç†ã«é€²ã‚ã¾ã™")


def process_detected_highlights(url: str, highlights: list, use_gemini: bool = False,
                                use_papago: bool = False, auto_learn: bool = False):
    """æ¤œå‡ºã•ã‚ŒãŸãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’å‡¦ç†"""
    if not highlights:
        print("å‡¦ç†ã™ã‚‹ãƒã‚¤ãƒ©ã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
        return

    print(f"\n=== {len(highlights)}ä»¶ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’å‡¦ç† ===")

    for i, h in enumerate(highlights):
        start_time = f"{int(h['start'] // 60)}:{int(h['start'] % 60):02d}"
        end_time = f"{int(h['end'] // 60)}:{int(h['end'] % 60):02d}"

        print(f"\n[{i+1}/{len(highlights)}] {h['title']}")
        print(f"   åŒºé–“: {start_time} - {end_time}")

        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        safe_title = "".join(c for c in h['title'][:20] if c.isalnum() or c in "_ -ã‚-ã‚“ã‚¢-ãƒ³ä¸€-é¾¥").strip()
        if not safe_title:
            safe_title = f"clip_{i+1}"
        output_name = f"highlight_{i+1:02d}_{safe_title}.mp4"

        create_clip_with_speaker(
            url=url,
            start_time=start_time,
            end_time=end_time,
            output_name=output_name,
            use_gemini=use_gemini,
            use_papago=use_papago,
            auto_learn_mode=auto_learn
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='YouTubeåˆ‡ã‚ŠæŠœã + è©±è€…è­˜åˆ¥ + è‡ªå‹•å­¦ç¿’ + Geminiç¿»è¨³')
    parser.add_argument('url', nargs='?', help='YouTube URL')
    parser.add_argument('start', nargs='?', help='é–‹å§‹æ™‚é–“')
    parser.add_argument('end', nargs='?', help='çµ‚äº†æ™‚é–“')
    parser.add_argument('--team', '-t', help='ãƒãƒ¼ãƒ æŒ‡å®š')
    parser.add_argument('--output', '-o', help='å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å')
    parser.add_argument('--gemini', '-g', action='store_true', help='Gemini APIã§é«˜ç²¾åº¦ç¿»è¨³ï¼ˆæ–‡è„ˆä»˜ãï¼‰')
    parser.add_argument('--papago', '-p', action='store_true', help='Papagoç¿»è¨³ï¼ˆéŸ“æ—¥ç‰¹åŒ–ã€æ¨å¥¨ï¼‰')
    parser.add_argument('--merge', '-m', action='store_true', help='ä¼šè©±ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆã—ã¦è‡ªç„¶ãªå­—å¹•ã«ï¼‰')
    parser.add_argument('--style', default='pro', choices=['pro', 'simple'], help='å­—å¹•ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆpro=ãƒ—ãƒ­é¢¨é»„è‰², simple=ã‚·ãƒ³ãƒ—ãƒ«ï¼‰')
    parser.add_argument('--detect', '-d', action='store_true', help='ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--learn', '-l', action='store_true', help='å¯¾è©±çš„å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--auto-learn', '-a', action='store_true', help='è‡ªå‹•å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆé«˜ã‚¹ã‚³ã‚¢ã®ã¿ï¼‰')
    parser.add_argument('--correct', '-c', action='store_true', help='ç›´è¿‘ã®çµæœã‚’ä¿®æ­£')
    parser.add_argument('--improve', '-i', help='æŒ‡å®šãƒãƒ¼ãƒ ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’è‡ªå‹•æ”¹å–„ï¼ˆT1, GenG, HLE, DK, KTï¼‰')
    parser.add_argument('--no-auto-improve', action='store_true', help='è‡ªå‹•å“è³ªæ”¹å–„ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--quality', '-q', action='store_true', help='ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å“è³ªã‚’ãƒã‚§ãƒƒã‚¯')
    parser.add_argument('--backup', action='store_true', help='ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—')
    parser.add_argument('--restore', nargs='?', const='latest', help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ')
    parser.add_argument('--list-backups', action='store_true', help='ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§')
    parser.add_argument('--no-auto-team', action='store_true', help='ãƒãƒ¼ãƒ è‡ªå‹•æ¤œå‡ºã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--min-score', type=float, default=0.3, help='æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰ã®æœ€å°ã‚¹ã‚³ã‚¢')

    # ç²¾åº¦å‘ä¸Šã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--separate', '-s', action='store_true',
                        help='éŸ³å£°åˆ†é›¢ï¼ˆDemucsï¼‰ã§ã‚²ãƒ¼ãƒ éŸ³ã‚’é™¤å»')
    parser.add_argument('--whisper-model', '-w', default='medium',
                        choices=['tiny', 'base', 'small', 'medium', 'large'],
                        help='Whisperãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: mediumã€å¤§ãã„ã»ã©é«˜ç²¾åº¦ã ãŒé…ã„ï¼‰')
    parser.add_argument('--high-quality', '-hq', action='store_true',
                        help='é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ï¼ˆ--gemini --separate --whisper-model medium ã‚’æœ‰åŠ¹åŒ–ï¼‰')

    # ç¿»è¨³å­¦ç¿’ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    parser.add_argument('--add-fix', nargs=2, metavar=('WRONG', 'CORRECT'),
                        help='ç¿»è¨³ä¿®æ­£ã‚’å­¦ç¿’ï¼ˆä¾‹: --add-fix "æ‚Ÿç©º" "ã‚¾ãƒ¼ãƒ‹ãƒ£"ï¼‰')
    parser.add_argument('--list-fixes', action='store_true',
                        help='å­¦ç¿’æ¸ˆã¿ç¿»è¨³ä¿®æ­£ã®ä¸€è¦§')
    parser.add_argument('--remove-fix', metavar='WRONG',
                        help='å­¦ç¿’æ¸ˆã¿ç¿»è¨³ä¿®æ­£ã‚’å‰Šé™¤')

    args = parser.parse_args()

    # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.high_quality:
        args.gemini = True
        args.separate = True
        args.whisper_model = 'medium'

    # è‡ªå‹•æ”¹å–„ã®ç„¡åŠ¹åŒ–
    if args.no_auto_improve:
        AUTO_IMPROVE_CONFIG['enabled'] = False

    # ç¿»è¨³å­¦ç¿’ã‚³ãƒãƒ³ãƒ‰
    if args.add_fix:
        from lol_dictionary import save_learned_correction
        wrong, correct = args.add_fix
        save_learned_correction(wrong, correct, context="manual")
        sys.exit(0)
    elif args.list_fixes:
        from lol_dictionary import list_learned_corrections
        fixes = list_learned_corrections()
        if not fixes:
            print("\nğŸ“š å­¦ç¿’æ¸ˆã¿ç¿»è¨³ä¿®æ­£ã¯ã‚ã‚Šã¾ã›ã‚“")
        else:
            print(f"\nğŸ“š å­¦ç¿’æ¸ˆã¿ç¿»è¨³ä¿®æ­£ ({len(fixes)}ä»¶)")
            print("=" * 60)
            for i, fix in enumerate(fixes):
                print(f"  [{i+1}] ã€Œ{fix['wrong']}ã€â†’ã€Œ{fix['correct']}ã€")
                if fix.get('context'):
                    print(f"       æ–‡è„ˆ: {fix['context']}")
                print(f"       æ—¥æ™‚: {fix['timestamp'][:16]}")
        sys.exit(0)
    elif args.remove_fix:
        from lol_dictionary import remove_learned_correction
        remove_learned_correction(args.remove_fix)
        sys.exit(0)

    if args.list_backups:
        if not AUTO_COLLECT_AVAILABLE:
            print("âŒ auto_collect_voice ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        else:
            backups = list_backups()
            if not backups:
                print("\nğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¯ã‚ã‚Šã¾ã›ã‚“")
            else:
                print(f"\nğŸ’¾ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä¸€è¦§ ({len(backups)}ä»¶)")
                print("=" * 60)
                for i, b in enumerate(backups):
                    print(f"  [{i+1}] {b['name']}")
                    print(f"      æ—¥æ™‚: {b['timestamp']}")
                    print(f"      ç†ç”±: {b['reason']}")
                    print(f"      ãƒ•ã‚¡ã‚¤ãƒ«: {len(b.get('files', []))}å€‹")
    elif args.backup:
        if not AUTO_COLLECT_AVAILABLE:
            print("âŒ auto_collect_voice ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        else:
            print("\nğŸ’¾ æ‰‹å‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆä¸­...")
            create_backup(reason="manual")
            print("âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†")
    elif args.restore:
        if not AUTO_COLLECT_AVAILABLE:
            print("âŒ auto_collect_voice ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        else:
            backup_name = None if args.restore == 'latest' else args.restore
            print(f"\nğŸ”„ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒä¸­...")
            restore_backup(backup_name)
    elif args.correct:
        correct_last_session()
    elif args.quality:
        # å“è³ªãƒã‚§ãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰
        team = args.team or args.improve
        quality = check_embedding_quality(team)
        print(f"\n{'='*60}")
        print(f"ğŸ“Š ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å“è³ªãƒ¬ãƒãƒ¼ãƒˆ" + (f" ({team})" if team else ""))
        print(f"{'='*60}")
        print(f"   ğŸŸ¢ é«˜å“è³ª: {len(quality['high'])}äºº")
        for p in quality['high']:
            print(f"      - {p['name']} (avg: {p['score']:.3f})")
        print(f"   ğŸŸ¡ ä¸­å“è³ª: {len(quality['medium'])}äºº")
        for p in quality['medium']:
            print(f"      - {p['name']} (avg: {p['score']:.3f})")
        print(f"   ğŸ”´ ä½å“è³ª: {len(quality['low'])}äºº")
        for p in quality['low']:
            print(f"      - {p['name']} (avg: {p['score']:.3f})")
        print(f"   âšª æ–°è¦: {len(quality['new'])}äºº")
        for p in quality['new']:
            print(f"      - {p}")
        if quality['low'] or quality['new']:
            print(f"\nğŸ’¡ æ”¹å–„ãŒå¿…è¦ãªé¸æ‰‹ãŒã„ã¾ã™ã€‚ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§è‡ªå‹•æ”¹å–„:")
            team_arg = f" --team {team}" if team else ""
            print(f"   python clip_with_speaker.py --improve{team_arg}")
    elif args.improve:
        # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ”¹å–„ãƒ¢ãƒ¼ãƒ‰
        improve_embeddings(args.improve)
    elif args.detect and args.url:
        # ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºãƒ¢ãƒ¼ãƒ‰
        highlights = detect_teamvoice_highlights(args.url, min_score=args.min_score)
        if highlights:
            print("\nå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n): ", end="")
            try:
                response = input().strip().lower()
                if response == 'y':
                    process_detected_highlights(
                        args.url, highlights,
                        use_gemini=args.gemini,
                        use_papago=args.papago,
                        auto_learn=args.auto_learn
                    )
            except EOFError:
                # éå¯¾è©±ç’°å¢ƒã§ã¯å…¨å‡¦ç†
                process_detected_highlights(
                    args.url, highlights,
                    use_gemini=args.gemini,
                    use_papago=args.papago,
                    auto_learn=args.auto_learn
                )
    elif args.url and args.start and args.end:
        create_clip_with_speaker(
            url=args.url,
            start_time=args.start,
            end_time=args.end,
            team=args.team,
            output_name=args.output,
            auto_detect_team=not args.no_auto_team,
            learn_mode=args.learn,
            auto_learn_mode=args.auto_learn,
            use_gemini=args.gemini,
            use_papago=args.papago,
            separate_audio=args.separate,
            whisper_model=args.whisper_model,
            merge_mode=args.merge,
            subtitle_style=args.style
        )
    else:
        parser.print_help()
        print("\nä½¿ç”¨ä¾‹:")
        print("  # é€šå¸¸å‡¦ç†ï¼ˆæŒ‡å®šåŒºé–“ã‚’åˆ‡ã‚ŠæŠœãï¼‰")
        print("  python clip_with_speaker.py 'https://youtube.com/...' 12:33 13:06")
        print()
        print("  # é«˜å“è³ªãƒ¢ãƒ¼ãƒ‰ï¼ˆGeminiç¿»è¨³ + éŸ³å£°åˆ†é›¢ + ä¸­å‹Whisperï¼‰")
        print("  python clip_with_speaker.py 'https://youtube.com/...' 12:33 13:06 --high-quality")
        print()
        print("  # å€‹åˆ¥ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        print("  python clip_with_speaker.py 'https://youtube.com/...' 12:33 13:06 --gemini --separate -w medium")
        print()
        print("  # ãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹è‡ªå‹•æ¤œå‡ºï¼ˆé•·ã„å‹•ç”»ã‹ã‚‰ç››ã‚Šä¸ŠãŒã‚Šéƒ¨åˆ†ã‚’æ¤œå‡ºï¼‰")
        print("  python clip_with_speaker.py 'https://youtube.com/...' --detect")
        print()
        print("  # å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰")
        print("  python clip_with_speaker.py 'https://youtube.com/...' 12:33 13:06 --learn")
        print()
        print("  # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å“è³ªãƒã‚§ãƒƒã‚¯")
        print("  python clip_with_speaker.py --quality")
        print("  python clip_with_speaker.py --quality --team T1")
        print()
        print("  # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•æ”¹å–„ï¼ˆãƒãƒ¼ãƒ ãƒœã‚¤ã‚¹å‹•ç”»ã‹ã‚‰è©±è€…ç…§åˆã§åé›†ï¼‰")
        print("  python clip_with_speaker.py --improve T1")
